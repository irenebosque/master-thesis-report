\chapter{Introduction}
\label{chapter:introduction}

Robots are increasingly present in our society; from factories to operating theatres, the demand for robots that are able to quickly adapt to changing environments does not stop growing. This necessity of fast adaptation to new situations makes hard-coding control strategies less suitable, and requires other types of flexible control approaches \cite{need-of-flexible-control-approaches}. Reinforcement learning (RL) is one of these flexible approaches where an intelligent agent tries to learn how to perform a task by maximizing a sum of rewards in a trial and error manner \cite{Sutton:1998}. RL techniques have been applied in successful cases such as \cite{Atari-RL}, \cite{alphaGO-silver-2016}, \cite{openAI-hand}, however, these examples tend to happen in simulated environments with very specific learning tasks. This fact greatly differs from real-world situations such in robotics where, in order to learn a good policy, a robot agent requires a huge amount of data, for which it will have to perform thousands of trials  \cite{reinforcement-learning-costly-Kober:2013}. This is very costly both in terms of time and the probable physical damages caused to the robot while learning \cite{TAMER-Knox-Stone:2009}. Furthermore, many real problems are easier to demonstrate than to design a reward function for applying reinforcement learning \cite{kostrikov2019imitation}. In fact, the incorporation of human knowledge in the learning process results in dramatically more efficient methods compared to autonomous learning techniques \cite{Global-overview-Attia:2018}. Imitation learning (IL) is the name of the field that leverages the knowledge of a teacher to improve the learning process. The simplest form of IL is known as behavioural cloning (BC) where an expert provides initial demonstrations of a desired task and then, an agent tries to imitate that behaviour via supervised learning.  Behavioural cloning has two main drawbacks: First, it requires demonstrations from an expert teacher which limits the possibilities of who can train the agent. And secondly, it suffers from distribution mismatch, a problem that initiates at the moment the agent deviates from the expert trajectory causing a cascade of errors that will probably make the agent fail the task.

\setlength{\parskip}{1em}

Interactive imitation learning (IIL) is a branch of imitation learning \cite{lazydagger:2021} that deals with the aforementioned issues by allowing a teacher to supervise and teach an agent \textit{during} its training. The nature of the feedback varies between frameworks; feedback in the form of evaluations (e.g., the TAMER framework \cite{TAMER-Knox-Stone:2009}) inform the agent how good or bad was the action taken. This kind of evaluative feedback is easier to implement than to define a reward function and allows a faster convergence than pure autonomous learning. However, the informativeness of this kind of evaluative feedback is still limited \cite{types-feedback-najar:2020}, and one way to improve it is to use corrections. Corrective imitation learning (CIL) is a branch of IIL that improves the informativeness of evaluative feedback, by allowing the teacher to inform the agent whether the value of a taken action should be increased or decreased \cite{Relative-corrections-Celemin:2019} and it requires less exploration compared to evaluative feedback \cite{types-feedback-najar:2020}.

\setlength{\parskip}{1em}



The goal of this master thesis is to create an extension of D-COACH (Deep COrrective Advice Communicated by Humans) \cite{ResearchAssignmentpaper}, a CIL algorithm designed for non-expert humans that uses neural networks to approximate the policy. This extension focuses on the improvement of a key component in several deep learning algorithms, experience replay \cite{Atari-RL}, a technique that uses experiences stored in a replay buffer for training. ER endows algorithms with two main advantages, these being a higher data efficiency and the ability to train with uncorrelated data \cite{Experience-Replay-zhang:2018}. These benefits are especially useful when policies are approximated with neural networks because already collected experiences can be reused multiple times and the neural network gets more robust against locally overfitting to the most recent trajectories, a phenomenon known as \textit{catastrophic forgetting}. The incorporation of ER into an algorithm requires that said algorithm is what in RL is known as off-policy \cite{Sutton:1998}. According to Sutton and Barto, off-policy learning occurs when a policy is updated with data gathered \say{off} that policy. The existing version of D-COACH is on-policy, because the policy is updated with data that depends on its most recent version. Despite being on-policy, current D-COACH uses experience replay for data-efficiency purposes. Given that the algorithm is on-policy, the size of the replay buffer has to be small, which works under the assumption that the data stored in the replay buffer is still valid for the current version of the policy, even if it was collected by an older version of itself. As the size of the replay buffer starts to increase, this assumption does not hold anymore and the training of the policy will most likely fail. If we want to have a more general framework, this restriction has to be removed and this is precisely the objective of this master thesis.




\begin{figure}[H]
    \centering
    \adjustbox{trim=0cm 0cm 0cm 0cm}{%
  \includesvg[width=.55\textwidth]{figures/IIL.svg}}
    \caption{Interactive Imitation Learning}
    \label{fig:IIL}
\end{figure}



\setlength{\parskip}{1em}

This report first presents the theoretical framework and relevant work in Chapter \ref{chapter:Theoretical Framework and Related Work}. Chapter \ref{chapter:Proposed Method} follows with the presentation of the proposed algorithm BD-COACH which is validated with the experiments, both in simulation and in a real setup explained in Chapter \ref{chapter:Experimental Setting}. Chapter \ref{chapter:Results} shows the results obtained with the new method and finally the report is ended with the relevant conclusions in Chapter \ref{chapter:conclusion}.

