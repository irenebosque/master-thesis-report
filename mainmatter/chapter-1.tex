\chapter{Introduction}
\label{chapter:introduction}

\setlength{\parskip}{0.7em}

We can envision our lives in a not too distant future where robots and intelligent agents make our existence easier. From kitchen robots to automated factories, the demand for technology that is able to quickly adapt to changing environments does not stop growing. This necessity of fast adaptation to new situations makes hard-coding control strategies less suitable and requires other types of flexible control approaches \cite{need-of-flexible-control-approaches}. Reinforcement learning (RL) is one of these flexible approaches where an intelligent agent tries to learn how to perform a task by maximizing a sum of rewards in a trial and error manner \cite{Sutton:1998}. RL techniques have been applied in successful cases such as \cite{Atari-RL}, \cite{alphaGO-silver-2016}, \cite{openAI-hand}, however, these examples tend to happen in simulated environments with very specific learning tasks. This fact greatly differs from real-world situations such in robotics where, in order to learn a good policy, a robot agent requires a huge amount of data, for which it will have to perform thousands of trials  \cite{reinforcement-learning-costly-Kober:2013}. This is very costly both in terms of time and the probable physical damages caused to the robot and its surroundings while learning \cite{TAMER-Knox-Stone:2009}. Furthermore, many real problems are easier to demonstrate than to design a reward function for applying reinforcement learning \cite{kostrikov2019imitation}. In fact, the incorporation of human knowledge in the learning process results in dramatically more efficient methods compared to autonomous learning techniques \cite{Global-overview-Attia:2018}. Imitation learning (IL) is the name of the field that leverages the knowledge of a teacher to improve the learning process. The simplest form of IL is known as behavioural cloning (BC) where an expert provides initial demonstrations of the desired task and then, an agent tries to imitate that behaviour via supervised learning. Behavioural cloning has two main drawbacks: First, it requires demonstrations from an expert teacher which limits the possibilities of who can train the agent. And secondly, it suffers from covariate shift, a distribution mismatch problem that initiates at the moment the agent deviates from the expert trajectory causing a cascade of errors that will probably make the agent fail the task.



% Interactive imitation learning (IIL) is a branch of imitation learning \cite{lazydagger:2021} that deals with the aforementioned issues by allowing a teacher to supervise and teach an agent \textit{during} its training, see Figure \ref{fig:IIL}. The nature of the feedback varies between frameworks. Feedback in the form of evaluations (e.g., the 
% Training an Agent Manually via Evaluative Reinforcement, TAMER framework \cite{TAMER-Knox-Stone:2009}) inform the agent how good or bad was the action taken. This kind of evaluative feedback is easier to implement than to define a reward function and allows a faster convergence than pure autonomous learning. However, the informativeness of this kind of evaluative feedback is still limited \cite{types-feedback-najar:2020}, and one way to improve it is to use corrections. Corrective imitation learning (CIL) is a branch of IIL that improves the informativeness of evaluative feedback, by allowing the teacher to inform the agent whether the value of a taken action should be increased or decreased \cite{Relative-corrections-Celemin:2019} and it requires less exploration compared to evaluative feedback \cite{types-feedback-najar:2020}.

% Interactive imitation learning (IIL) is a branch of imitation learning \cite{lazydagger:2021} that deals with the aforementioned issues by allowing a teacher to help the agent learn \textit{during} its training, see Figure \ref{fig:IIL}. There are different forms in which the human can communicate his/her knowledge to facilitate the learning process. One way is for the human to demonstrate the task when the robot request it \cite{demonstration-robot-query} for example by teleoperating the robot or by using kinesthetic teaching. Another type of feedback are evaluations. Evaluative feedback in the form of scalar values (e.g., the framework Training an Agent Manually via Evaluative Reinforcement, TAMER \cite{TAMER-Knox-Stone:2009}) inform the agent how good or bad was the action taken. Evaluative feedback can also be provided as preferences between demonstrated trajectories \cite{learning-from-human-preferences:2018}. Here the human is presented with several executions of a policy, and he/she has to decide which one is better according to the goal of the task. Then, a reward function that explains the decisions of the human is found, and by applying RL, the agent learns how to perform the task. This kind of evaluative feedback is easier to implement than to define a reward function and allows a faster convergence than pure autonomous learning. However, the informativeness of this kind of evaluative feedback is still limited \cite{types-feedback-najar:2020}, and one way to improve it is to use corrections. Corrective imitation learning (CIL) is a branch of IIL that improves the informativeness of evaluative feedback, by allowing the teacher to inform the agent whether the value of a taken action should be increased or decreased \cite{COACH-Celemin-Ruiz-del-Solar:2015} and it requires less exploration compared to evaluative feedback \cite{types-feedback-najar:2020}.

Interactive imitation learning (IIL) is a branch of imitation learning \cite{lazydagger:2021} that deals with the aforementioned issues by allowing a teacher to help the agent learn \textit{during} its training, see Figure \ref{fig:IIL}. The human can communicate his/her knowledge using different types of feedback, such as demonstrations when requested by the robot \cite{demonstration-robot-query}, evaluations that can be scalar values \cite{TAMER-Knox-Stone:2009} or preferences \cite{learning-from-human-preferences:2018}, corrections \cite{COACH-Celemin-Ruiz-del-Solar:2015}, and so forth. In this work, we focus on corrections which gives name to the branch of IIL called corrective imitation learning (CIL). In CIL frameworks the human teacher sends corrections informing the agent whether the value of a taken action should be increased or decreased \cite{Relative-corrections-Celemin:2019}.








\begin{figure}[H]
    \centering
    \adjustbox{trim=0cm 0cm 0cm 0cm}{%
  \includesvg[width=.55\textwidth]{figures/IIL.svg}}
    \caption{Interactive Imitation Learning}
    \label{fig:IIL}
\end{figure}




The goal of this master thesis is to create an extension of Deep COrrective Advice Communicated by Humans, D-COACH \cite{D-COACH-Dattari-Celemin-Ruiz-del-Solar-Kober:2018}, a CIL algorithm designed for non-expert humans that uses an artificial neural network as a function approximator for its policy. This extension focuses on the improvement of a key component in several deep learning algorithms in sequential decision-making problems, experience replay (ER) \cite{Atari-RL}, a technique that leverages past experiences stored in a replay buffer. ER endows algorithms with two main advantages, these being a higher data efficiency and the ability to train with uncorrelated data \cite{Experience-Replay-zhang:2018}. These benefits are especially useful when policies are approximated with artificial neural networks (ANNs). Collected past experiences can be reused multiple times and the ANN gets more robust against locally overfitting to the most recent trajectories, a phenomenon known as \textit{catastrophic forgetting}. Note that we refer to ER as \textit{corrections replay} since, in this work, we replay old corrective feedback. In RL, only off-policy frameworks, those that can update their current policies with data gathered by different policies, are able to correctly employ ER \cite{Atari-RL}. As this is related to the problem we want to solve, in this work, we explore and analyze the terms on-policy and off-policy in imitation learning.




D-COACH already incorporates corrections replay for data-efficiency purposes. However, the way it is implemented, causes ambiguities to arise between the data inside the buffer. This forces to limit the size of the buffer which works under the assumption that the data stored in the replay buffer is still valid for the current version of the policy, even if it was collected by an older version of the policy. As the size of the replay buffer starts to increase, this assumption does not hold anymore and the training of the policy will most likely fail, limiting therefore, the types of problems that D-COACH can address. If we want to utilise a buffer to train data intensive tasks with corrective feedback, a new method is needed to solve the problem of using corrections gathered by older versions of the policy. To address this challenge, we propose Batch Deep COACH (BD-COACH, pronounced \say{be the coach}). BD-COACH incorporates a human model module that learns the feedback from the teacher and that can be employed to make corrections gathered by older versions of the policy still useful for batch updating the current version of the policy.





\section{Objectives}
\label{section:aim}


The aim of this master thesis can be divided in the following objectives:


\begin{itemize}
  \item Define and implement a framework that, by using deep neural network policies, allows robots to learn data intensive tasks by replaying old corrective feedback given by a human teacher from a replay buffer.
 

  \item Validate the proposed method in simulated experiments.
 \item Validate the proposed method using a real robotic platform.
\end{itemize}




\section{Outline}
\label{section:outline}




This work first presents the theoretical framework and relevant work in Chapter \ref{chapter:Theoretical Framework and Related Work}. Chapter \ref{chapter:Proposed Method} follows with the presentation of the proposed algorithm BD-COACH which is validated with the experiments, both in simulation and in a real setup explained in Chapter \ref{chapter:Experimental Setting}. Chapter \ref{chapter:Results} shows the results obtained with the new method and finally, the report ends in Chapter \ref{chapter:conclusion} with the relevant conclusions.

