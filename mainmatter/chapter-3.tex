\chapter{Batch Deep COACH (BD-COACH)}
\label{chapter:Proposed Method}



The current version of the corrective interactive learning algorithm D-COACH is limited to problems that do not require large amounts of data as its replay buffer needs to be kept small, see Section \ref{al:D-COACH}. A new method is therefore needed in order to maximise the full potential of the replay buffer which will allow to train for tasks that are more data demanding. In this chapter we present the method Batch Deep COACH (BD-COACH) a robust version of D-COACH that is able to perform policy dependent updates by incorporating a  human model module that learns the feedback from the teacher.

Section \ref{section:Difference between D-COACH and BD-COACH} focuses on the difference between D-COACH and BD-COACH.

Section \ref{section:Proposed method: BD-COACH} explains the modules that form the agent of BD-COACH.


Finally, section \ref{section:Algorithm-Discussion} gathers the conclusions about the new proposed method.  


\section{Difference between D-COACH and BD-COACH}
\label{section:Difference between D-COACH and BD-COACH}

As presented in Section \ref{subsection:D-COACH}, D-COACH is limited to problems that do not require big datasets for training. In order to avoid catastrophic forgetting and overfitting, D-COACH makes use of a replay buffer whose size is kept small to avoid unuseful corrections to be conveyed to the current version of the policy for its update. This limitation makes  D-COACH perform worst when facing tasks that require a lot of data. There are two aspects that determine the amount of data required for training a task. First, the dimensionality of the observation;  the higher the dimensionality of the observation is, the more data is needed. And secondly, the horizon of the task; the longer the horizon, the more data is required. Therefore a task with a combination of a high dimensionality and a long horizon would be very challenging for D-COACH to learn. In D-COACH the updates are independent of the policy as corrections from the buffer do not depend on what the policy is currently doing at those particular states; by doing so, feedback gathered by older versions of the policy may not be useful anymore.


In order to be able to work  with higher demanding data tasks, we propose BD-COACH a new method with policy dependent updates. This is achieved by incorporating a human model that learns to predict the feedback that the teacher provides and which depends on the output of the policy at a particular state. The human model learns to filter from the buffer using only what is convenient for the current policy. Table \ref{tab:difference-DCOACH-RCIDL} shows a summary of the dependencies of both methods.
               

\begin{table}[H]
\centering
\renewcommand{\arraystretch}{1.4}
\begin{tabular}{lS[table-format=4]
                 lS[table-format=4]
                 lS[table-format=4]}
\toprule
Method  & {Feedback is:} & {Batch updates are:}\\[-.4em]
\midrule
\textbf{D-COACH}  &   {Policy dependent} &   {Policy independent}\\
\textbf{BD-COACH}  &  {Policy dependent} &   {Policy dependent}\\
\bottomrule
\end{tabular}
\caption{Difference between D-COACH and BD-COACH}
\label{tab:difference-DCOACH-RCIDL}
\end{table}

Notice that for both methods, corrections are always dependent of the policy as the human teacher provides corrections depending on what the policy is doing. 







\section{Proposed method: BD-COACH}
\label{section:Proposed method: BD-COACH}

Figure \ref{fig:RCIdL-agent} depicts the modules that compose the agent of the proposed method BD-COACH. The coloured arrows other than black identify the different updates of the policy and the human model. Specifically, the red arrows show the flow of information for a single update of the policy while the green ones are used for the batch update of the same policy. Finally, the blue arrows indicate a batch update of the human model. 



\begin{figure}[H]
    \centering
    \adjustbox{trim=0cm 0cm 0cm 0cm}{%
  \includesvg[width=.9\textwidth]{figures/BD-COACH-diagram.svg}}
    \caption{Modules of the BD-COACH agent}
    \label{fig:RCIdL-agent}
\end{figure}


\subsection*{Policy}

The policy $P(s)$ is the core of the agent that takes decisions in an environment. This function, represented in BD-COACH by a neural network, observes the state of the environment and emits an action accordingly. The policy is learnt during training with both single updates and batch updates. 


\subsection*{Replay buffer}
The replay buffer $\mathcal{B}$ is an artificial memory  used to store the experience gathered by the policy on the time steps when the human teacher provides corrections. This information consists on a 3-tuple formed by the state of the environment at time step $t$,  $s_t$, the correspondent action from the policy, $a_t$ and the feedback signal from the human, $h_t$. 

\subsection*{Policy update module}
This module is in charge of updating the policy both with single and batch updates.
Single updates happen when at a particular time step during the training, the human teacher considers that the action emitted by the policy, $P(s) = a$, should be modified, and provides a feedback signal $h$ to correct the action in the desired direction. This signal is fed directly to the policy update module (red arrow of Figure \ref{fig:RCIdL-agent}) to perform a single update of the weights $\theta$ of the policy. Specifically, the policy update module, follows a SGD optimization strategy to find the network parameters $\theta$ that minimize the difference between the actual action and the target action. Because knowing the exact value of the desired action can be hard for a human teacher, he/she simply provides $h$ which is the direction of the correction. To compute the magnitude of the error, $h$ is multiplied by the hyperparameter $e$ that is chosen beforehand according to the task, $\text{error}_t = h_t \cdot  e$. The value of the desired action can then be obtained by adding the error to the actual action from the policy as shown in Equation \eqref{eq:a-target}:

\begin{equation}
a^\text{target}_t = a_t + \text{error}_t
\label{eq:a-target}
\end{equation}


The mean of the square of the difference between the target action and the actual action is used as the cost function $J(\theta)$ that is minimized by the SGD optimizer. Equation \eqref{eq:first-update-rule} shows the update rule of the main policy where $\alpha$ is the learning rate.


\begin{equation}
\theta \leftarrow \theta - \alpha \cdot \nabla_\theta J(\theta)
\label{eq:first-update-rule}
\end{equation}

On the other hand, batch updates (green loop in Figure \ref{fig:RCIdL-agent} happen every $b$ time steps regardless of whether the teacher has give correction at that time step or not. Is in this batch update where the main difference with D-COACH can be observed as the batch of corrections that enter the policy update module depend on the actions taken by the policy for those states.




\subsection*{Human model}
The human model, $H(s, a) = \hat{h}$, learns to predict the corrective feedback given by the human teacher for batches of state-action pairs. This model is represented by a neural network and it is learnt in parallel together with the policy during the training. This module the key in the batch update of the policy as it is able to provide feedback to the newest version of the policy, green loop in Figure \ref{fig:RCIdL-agent}.

During the batch update of the policy, a batch of states sampled from the replay buffer is passed to both the policy and the human model. The policy outputs a batch of actions that are fed to the human model. This batch of actions together with the output $\hat{h}$ from the human model, serve to update the weights of the policy in the policy update module. 



\subsection*{Human model update}

The human model update module is in charge of updating the weights of the network that represents the human model, $H_\omega$. The human model is updated with tuples of information $(s_t, a_t, h_t)$ stored in the replay buffer, applying like this the corrections replay technique. Specifically, batches of states and actions are feed to the human model which outputs a bath of $\hat{h}$ predictions. This, together with the correspondent batch of feedback from the buffer, is used to update the weights of the human model.

\subsection*{Complete algorithm}

% \begin{algorithm}[H]
% \caption{BD-COACH}\label{algorithm:BD-COACH}
% \begin{algorithmic}[1]
% \State \textbf{Require:} error magnitude $e$, buffer update interval $b$
% \State \textbf{Init:} $\mathcal{D} = [\quad]$  \emph{\# initialize memory buffer}
% \For{t = 1,2,...}{}
% \State \textbf{observe} state $o_{t}$
% \State \textbf{execute} action $a_{t}=\pi_{\theta}(o_{t})$
% \State \textbf{feedback} human corrective advice $h_{t}$
% \If{$h_{t}$ is not \textbf{0}}
% \State \textbf{append} $(o_{t}, a_{t}, h_{t})$ to $\mathcal{D}$
% \State $\text{error}_{t} = h_{t}\cdot e$
% \State $a_{\text{target}(t)} = a_{t} + \text{error}_{t}$
% \State \textbf{update} $\pi_\theta$ using SGD with pair ($o_{t}$, $a^{\text{target}}_{t}$)
% \State \textbf{update} $H_\omega$ using SGD with a mini-batch sampled from $\mathcal{D}$
% \State  $a=\pi_{\theta}(o)$ \emph{\# where $o$ is a batch of observations sampled from $\mathcal{D}$}
% \State  $\hat{h}=H_\omega(a, o)$
% \State \textbf{update} $\pi_\theta$ using SGD with batch $a$ and batch $\hat{h}$
% \EndIf
% \If{mod(t, b) is 0 }
% \State \textbf{update} $H_\omega$ using SGD with a mini-batch sampled from $\mathcal{D}$
% \State  $a=\pi_{\theta}(o)$ \emph{\# where $o$ is a batch of observations sampled from $\mathcal{D}$}
% \State  $\hat{h}=H_\omega(a, o)$
% \State \textbf{update} $\pi_{\theta}$ using SGD with batch $a$ and batch $\hat{h}$
% \EndIf
% \EndFor
% \end{algorithmic}
% \label{al:D-COACH with HM}
% \end{algorithm}



\begin{algorithm}[H]
\caption{BD-COACH}\label{algorithm:BD-COACH}
\begin{algorithmic}[1]
\State \textbf{Require:} error magnitude $e$, buffer update interval $b$
\State \textbf{Init:} $\mathcal{D} = [\quad]$  \emph{\# initialize memory buffer}
\For{t = 1,2,...}{}
\State \textbf{observe} state $o_{t}$
\State \textbf{execute} action $a_{t}=\pi_{\theta}(o_{t})$
\State \textbf{feedback} human corrective advice $h_{t}$
\If{$h_{t}$ is not 0}
\State \textbf{append} $(o_{t}, a_{t}, h_{t})$ to $\mathcal{D}$
\State $\text{error}_{t} = h_{t}\cdot e$
\State $a_{\text{target}(t)} = a_{t} + \text{error}_{t}$
\State \textbf{update} $\pi_\theta$ using SGD with pair ($o_{t}$, $a^{\text{target}}_{t}$)
\EndIf
\If{ $h_t$ {not 0 OR mod(t, b) is 0 }}
\State \textbf{update} $H_\omega$ using SGD with a mini-batch uniformly sampled from $\mathcal{D}$
\State  $a=\pi_{\theta}(o)$ \emph{\# where $o$ is a batch of observations uniformly sampled from $\mathcal{D}$}
\State  $\hat{h}=H_\omega(a, o)$
\State \textbf{update} $\pi_{\theta}$ using SGD with batch $a$ and batch $\hat{h}$
\EndIf
\EndFor
\end{algorithmic}
\label{al:D-COACH with HM}
\end{algorithm}










\section{Discussion}
\label{section:Algorithm-Discussion}
This chapter has explained in detail the proposed method BD-COACH. This new framework is similar to the state-of-the-art algorithm D-COACH and it can be used by non-expert human teachers to accelerate the learning of an agent when facing a new task that requires a big dataset. Through corrective feedback signals, the teacher modifies the actions taken by the agent if needed. We saw that the main difference between BD-COACH and D-COACH lies in the policy dependency of the batch updates of the policy. Whereas the batch updates of D-COACH are policy independent, the batch updates in BD-COACH depend on what the policy does. This is achieved by incorporating a human model module that learns to predict feedback by replaying corrections stored in a buffer. Removing the limitation of the buffer size $K$, makes BD-COACH more robust as its performance does not depend anymore on the values of $K$ and the parameter $e$.

However BD-COACH has its limitations. Computationally speaking, it is more expensive than D-COACH as two models are learnt in parallel. Regarding performance, with BD-COACH, it is minimally affected at the beginning of the training when the human model has not learn yet to predict suitable feedback.
% To ameliorate this issue two measures are taken. First, batch updates of the main policy start to take place when a minimum of $m$ signals of feedback from the teacher have been received. Meaning that the Human Model $H$ has been updated at least $m$ times


