%%%%%%%%%%%%%%%%%%%%%%%%% INSTRUCTIONS %%%%%%%%%%%%%%%%%%%%%%%%%

% Below you will be able to find a few examples for the most common types of references (articles, books and websites). Please ensure the reference follows the same format if you are writing it yourself. However, especially with articles, you might be better off finding the 'cite this reference' button on the website. 

% You can cite using \cite{name-of-entry}. The 'name-of-entry' is the part after the first line. For this for example below, citing would work like: \cite{Example-Article}. Once you have cited the source at least once in the text, the entry will appear in the bibliography at the end.

% For a cheat sheet with all entry types and much more, see http://tug.ctan.org/info/biblatex-cheatsheet/biblatex-cheatsheet.pdf

% Intorduction: Need of flexible approaches
@INPROCEEDINGS{need-of-flexible-control-approaches,

  author={L. {Nardi} and C. {Stachniss}},

  booktitle={2016 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)}, 

  title={Experience-based path planning for mobile robots exploiting user preferences}, 

  year={2016},

  volume={},

  number={},

  pages={1170-1176},

  doi={10.1109/IROS.2016.7759197}}
  

% openAI hand

@misc{openAI-hand,
      title={Learning Dexterous In-Hand Manipulation}, 
      author={OpenAI and Marcin Andrychowicz and Bowen Baker and Maciek Chociej and Rafal Jozefowicz and Bob McGrew and Jakub Pachocki and Arthur Petron and Matthias Plappert and Glenn Powell and Alex Ray and Jonas Schneider and Szymon Sidor and Josh Tobin and Peter Welinder and Lilian Weng and Wojciech Zaremba},
      year={2019},
      eprint={1808.00177},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

% alphaGO

@article{alphaGO-silver-2016,
  author = {Silver, David and Huang, Aja and Maddison, Chris J. and Guez, Arthur and Sifre, Laurent and van den Driessche, George and Schrittwieser, Julian and Antonoglou, Ioannis and Panneershelvam, Veda and Lanctot, Marc and Dieleman, Sander and Grewe, Dominik and Nham, John and Kalchbrenner, Nal and Sutskever, Ilya and Lillicrap, Timothy and Leach, Madeleine and Kavukcuoglu, Koray and Graepel, Thore and Hassabis, Demis},
  biburl = {https://www.bibsonomy.org/bibtex/29e987f58d895c490144693139cbc90c7/flint63},
  doi = {10.1038/nature16961},
  file = {Nature online:2016/SilverHuangEtAl16nature.pdf:PDF},
  groups = {public},
  interhash = {48430c7891aaf9fe2582faa8f5d076c1},
  intrahash = {9e987f58d895c490144693139cbc90c7},
  issn = {0028-0836},
  journal = {Nature},
  keywords = {01614 paper ai google learn algorithm},
  month = {#jan#},
  number = 7587,
  pages = {484--489},
  timestamp = {2018-04-16T12:03:12.000+0200},
  title = {Mastering the Game of {Go} with Deep Neural Networks and Tree Search},
  username = {flint63},
  volume = 529,
  year = 2016
}

% ATARI RL
@misc{Atari-RL,
      title={Playing Atari with Deep Reinforcement Learning}, 
      author={Volodymyr Mnih and Koray Kavukcuoglu and David Silver and Alex Graves and Ioannis Antonoglou and Daan Wierstra and Martin Riedmiller},
      year={2013},
      eprint={1312.5602},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}





@article{reinforcement-learning-costly-Kober:2013,
author = {Jens Kober and J. Andrew Bagnell and Jan Peters},
title ={Reinforcement learning in robotics: A survey},
journal = {The International Journal of Robotics Research},
volume = {32},
number = {11},
pages = {1238-1274},
year = {2013},
doi = {10.1177/0278364913495721},

URL = { 
        https://doi.org/10.1177/0278364913495721
    
},
eprint = { 
        https://doi.org/10.1177/0278364913495721
    
}

}

% Survey on types of feedback
@misc{types-feedback-najar:2020,
      title={Reinforcement learning with human advice: a survey}, 
      author={Anis Najar and Mohamed Chetouani},
      year={2020},
      eprint={2005.11016},
      archivePrefix={arXiv},
      primaryClass={cs.AI}
}

% Relative corrections Celemin
@article{Relative-corrections-Celemin:2019,
author = {Celemin, Carlos and Ruiz-Del-Solar, Javier},
title = {An Interactive Framework for Learning Continuous Actions Policies Based on Corrective Feedback},
year = {2019},
issue_date = {July      2019},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {95},
number = {1},
issn = {0921-0296},
url = {https://doi-org.tudelft.idm.oclc.org/10.1007/s10846-018-0839-z},
doi = {10.1007/s10846-018-0839-z},
journal = {J. Intell. Robotics Syst.},
month = jul,
pages = {77–97},
numpages = {21},
keywords = {Interactive machine learning, Learning from demonstration, Human feedback, Human teachers, Decision making systems}
}

  




% Sergey Levine Youtube
@online{youtube_offline_RL,
    title = {Offline Reinforcement Learning},
    year = {2020},
    organization = {University of Berkeley},
    author = {Sergey Levine},
    url = {https://www.youtube.com/watch?v=qgZPZREor5I},
    }
% Offline RL - Paper de Sergey Levine
@misc{Offline-RL-Levine:2020,
      title={Offline Reinforcement Learning: Tutorial, Review, and Perspectives on Open Problems}, 
      author={Sergey Levine and Aviral Kumar and George Tucker and Justin Fu},
      year={2020},
      eprint={2005.01643},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

Rishabh Agarwal

% Offline RL - Google
@online{Google_offline_RL,
    title = {An Optimistic Perspective on Offline Reinforcement Learning },
    year = {2020},
    organization = {Google Research},
    author = {Rishabh Agarwal},
    url = {https://ai.googleblog.com/2020/04/an-optimistic-perspective-on-offline.html},
    }

% A Deeper Look at Experience Replay
@misc{Experience-Replay-zhang:2018,
      title={A Deeper Look at Experience Replay}, 
      author={Shangtong Zhang and Richard S. Sutton},
      year={2018},
      eprint={1712.01275},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

% Neural Networks classification
@book{Classification-Artificial-Neural-Networks:2017,
  title={Neural Networks with R: Smart models using CNN, RNN, deep learning, and artificial intelligence principles},
  author={Ciaburro, G. and Venkateswaran, B.},
  isbn={9781788399418},
  url={https://books.google.es/books?id=IppGDwAAQBAJ},
  year={2017},
  publisher={Packt Publishing}
}
% Global overview of Imitation Learning
@misc{Global-overview-Attia:2018,
      title={Global overview of Imitation Learning}, 
      author={Alexandre Attia and Sharone Dayan},
      year={2018},
      eprint={1801.06503},
      archivePrefix={arXiv},
      primaryClass={stat.ML}
}

% Leveraging Human Guidance for Deep Reinforcement Learning Tasks



@article{leveraging-human-guidance:2019,
  author    = {Ruohan Zhang and
               Faraz Torabi and
               Lin Guan and
               Dana H. Ballard and
               Peter Stone},
  title     = {Leveraging Human Guidance for Deep Reinforcement Learning Tasks},
  journal   = {CoRR},
  volume    = {abs/1909.09906},
  year      = {2019},
  url       = {http://arxiv.org/abs/1909.09906},
  archivePrefix = {arXiv},
  eprint    = {1909.09906},
  timestamp = {Fri, 27 Sep 2019 13:04:21 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-1909-09906.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}



% Why sometimes is easier learning a policy than learning a reward
@misc{kostrikov2019imitation,
      title={Imitation Learning via Off-Policy Distribution Matching}, 
      author={Ilya Kostrikov and Ofir Nachum and Jonathan Tompson},
      year={2019},
      eprint={1912.05032},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}


% Original Q-learning paper
@PhdThesis{Watkins:1989,
  author =       "Watkins, Christopher John Cornish Hellaby",
  title =        "Learning from Delayed Rewards",
  school =       "King's College",
  year =         "1989",
  address =   "Cambridge, UK",
  month =     "May",
  url = "http://www.cs.rhul.ac.uk/~chrisw/new_thesis.pdf",
  bib2html_rescat = "Parameter",
}
% Original SARSA paper
@TechReport{Rummery+Niranjan:1994,
  author =       "Rummery, G. A. and Niranjan, M.",
  title =        "On-line {Q}-learning using connectionist systems",
  institution =  "Cambridge University Engineering Department",
  year =         "1994",
  type =      "CUED/F-INFENG/TR",
  number =    "166",
  month =     "September",
  bibdate =      "Thu Feb 10 16:53:24 1994",
  url = "ftp://svr-ftp.eng.cam.ac.uk/reports/rummery_tr166.ps.Z",
  bib2html_rescat = "Learning Methods, General RL",
}

% Book An Algorithmic Perspective on Imitation Learning where they say when was the first mention to off-policy IL

@BOOK{Osa:2018,  author={T. {Osa} and J. {Pajarinen} and G. {Neumann} and J. A. {Bagnell} and P. {Abbeel} and J. {Peters}},  booktitle={An Algorithmic Perspective on Imitation Learning},  year={2018},  volume={},  number={},  pages={},  doi={10.1561/2300000053}}


% FIrst mention of On-policy Off-policy in IMitation Learning
@article{DBLP:journals/corr/LaskeyLHLMFG17,
  author    = {Michael Laskey and
               Jonathan Lee and
               Wesley Yu{-}Shu Hsieh and
               Richard Liaw and
               Jeffrey Mahler and
               Roy Fox and
               Ken Goldberg},
  title     = {Iterative Noise Injection for Scalable Imitation Learning},
  journal   = {CoRR},
  volume    = {abs/1703.09327},
  year      = {2017},
  url       = {http://arxiv.org/abs/1703.09327},
  archivePrefix = {arXiv},
  eprint    = {1703.09327},
  timestamp = {Mon, 23 Nov 2020 08:36:40 +0100},
  biburl    = {https://dblp.org/rec/journals/corr/LaskeyLHLMFG17.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

% Biblia del RL

@book{Sutton:1998,
author = {Sutton, Richard S. and Barto, Andrew G.},
title = {Introduction to Reinforcement Learning},
year = {1998},
isbn = {0262193981},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
edition = {1st},
abstract = {From the Publisher:In Reinforcement Learning, Richard Sutton and Andrew Barto provide a clear and simple account of the key ideas and algorithms of reinforcement learning. Their discussion ranges from the history of the field's intellectual foundations to the most recent developments and applications. The only necessary mathematical background is familiarity with elementary concepts of probability.}
}

  


% Human-centric sampling vs Robot-centric sampling

@misc{laskey2017comparing,
      title={Comparing Human-Centric and Robot-Centric Sampling for Robot Deep Learning from Demonstrations}, 
      author={Michael Laskey and Caleb Chuck and Jonathan Lee and Jeffrey Mahler and Sanjay Krishnan and Kevin Jamieson and Anca Dragan and Ken Goldberg},
      year={2017},
      eprint={1610.00850},
      archivePrefix={arXiv},
      primaryClass={cs.RO}
}

% Laskey phd thesis
@phdthesis{Laskey:phdthesis,
    Author = {Laskey, Michael},
    Title = {On and Off-Policy Deep Imitation Learning for Robotics},
    School = {EECS Department, University of California, Berkeley},
    Year = {2018},
    Month = {Aug},
    URL = {http://www2.eecs.berkeley.edu/Pubs/TechRpts/2018/EECS-2018-108.html},
    Number = {UCB/EECS-2018-108},
    Abstract = {As an alternative to explicit programming for robots, Deep Imitation learning has two drawbacks: sample complexity and covariate shift.  One approach to Imitation Learning is Behavior Cloning, in which a
robot observes a supervisor and then infers a control policy. A known problem with this approach is that even slight departures from the supervisor’s demonstrations can compound over the policy’s roll-out resulting in errors; this concept of drift and resulting error is commonly referred to as covariate shift On-policy techniques reduce covariate shift by iteratively collecting corrective actions for the current robot policy.  To reduce sample complexity of these approaches, we propose a novel active learning algorithm, SHIV (Svm-based reduction in Human InterVention). While evaluating SHIV, we reconsider the trade-off between Off- and On-Policy methods and find that: 1) On-Policy methods are challenging for humans supervisors and 2) performance varies with the expressiveness of the policy class. To make Off-Policy methods more robust for expressive policies we propose a second algorithm, DART (Disturbances Augmenting Robot Trajectories), which injects optimized noise into the supervisor’s control stream to simulate error during data collection. This dissertation contributes two aforementioned algorithms, experimental evaluation with three robots evaluating their performance on tasks ranging from grasping in clutter to singulation to bed-making, and the design of a novel first-order urban driving simulator (FLUIDS) that can fill gaps in existing benchmarks for Imitation Learning to rapidly test algorithm performance in terms of generalization.}
}

% Research Assignment paper
@ARTICLE{ResearchAssignmentpaper,

  author={R. {Perez-Dattari} and C. {Celemin} and G. {Franzese} and J. {Ruiz-del-Solar} and J. {Kober}},

  journal={IEEE Robotics   Automation Magazine}, 

  title={Interactive Learning of Temporal Features for Control: Shaping Policies and State Representations From Human Feedback}, 

  year={2020},

  volume={27},

  number={2},

  pages={46-54},

  doi={10.1109/MRA.2020.2983649}}
  
  
  % Imition Learning definition
  @misc{Imitation-Learning-definition-torabi:2019,
      title={Recent Advances in Imitation Learning from Observation}, 
      author={Faraz Torabi and Garrett Warnell and Peter Stone},
      year={2019},
      eprint={1905.13566},
      archivePrefix={arXiv},
      primaryClass={cs.RO}
}
  
  
  % Behavioural Cloning - 1991
  @ARTICLE{Behavioural-Cloning-Pomerleau:1991,

  author={D. A. {Pomerleau}},

  journal={Neural Computation}, 

  title={Efficient Training of Artificial Neural Networks for Autonomous Navigation}, 

  year={1991},

  volume={3},

  number={1},

  pages={88-97},

  doi={10.1162/neco.1991.3.1.88}}

% SEARN - 2009
@misc{SEARN-DaumeIII:2009,
      title={Search-based Structured Prediction}, 
      author={Hal DauméIII and John Langford and Daniel Marcu},
      year={2009},
      eprint={0907.0786},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}
  
  % TAMER
  @InProceedings{TAMER-Knox-Stone:2009,
title={Interactively Shaping Agents via Human Reinforcement: The TAMER Framework},
author={W. Bradley Knox and Peter Stone},
booktitle={The Fifth International Conference on Knowledge Capture},
month={September},
url="http://www.cs.utexas.edu/users/ai-lab?KCAP09-knox",
year={2009}
}

% SMILe
 @InProceedings{SMILe-Ross-Bagnell:2010, title = {Efficient Reductions for Imitation Learning}, author = {Stephane Ross and Drew Bagnell}, booktitle = {Proceedings of the Thirteenth International Conference on Artificial Intelligence and Statistics}, pages = {661--668}, year = {2010}, editor = {Yee Whye Teh and Mike Titterington}, volume = {9}, series = {Proceedings of Machine Learning Research}, address = {Chia Laguna Resort, Sardinia, Italy}, month = {13--15 May}, publisher = {JMLR Workshop and Conference Proceedings}, pdf = {http://proceedings.mlr.press/v9/ross10a/ross10a.pdf}, url = {http://proceedings.mlr.press/v9/ross10a.html}} 
 
 % DAgger
 @misc{DAgger-Ross:2011,
      title={A Reduction of Imitation Learning and Structured Prediction to No-Regret Online Learning}, 
      author={S. Ross and G. Gordon and J. Bagnell},
      year={2011},
      eprint={1011.0686},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

% DAgger by coaching
@article{DAgger-by-coaching-He-DaumeIII-Eisner:2012,
author = {He, H. and III, H. and Eisner, J.},
year = {2012},
month = {01},
pages = {3149-3157},
title = {Imitation learning by coaching},
volume = {4},
journal = {Advances in Neural Information Processing Systems}
}

%Advise
@article{Advise-Griffith-et-al:2013,
author = {Griffith, S. and Subramanian, K. and Scholz, Jonathan and Isbell, C.L. and Thomaz, A.L.},
year = {2013},
month = {01},
pages = {},
title = {Policy shaping: Integrating human feedback with Reinforcement Learning},
journal = {Advances in Neural Information Processing Systems}
}

%AggreVaTe

@article{AggreVaTe-Ross-Bagnell:2014,
  author    = {S.Ross and
               J. Andrew Bagnell},
  title     = {Reinforcement and Imitation Learning via Interactive No-Regret Learning},
  journal   = {CoRR},
  volume    = {abs/1406.5979},
  year      = {2014},
  url       = {http://arxiv.org/abs/1406.5979},
  archivePrefix = {arXiv},
  eprint    = {1406.5979},
  timestamp = {Mon, 13 Aug 2018 16:47:16 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/RossB14.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}


% COACH

@INPROCEEDINGS{COACH-Celemin-Ruiz-del-Solar:2015,

  author={C. {Celemin} and J. {Ruiz-del-Solar}},

  booktitle={2015 International Conference on Advanced Robotics (ICAR)}, 

  title={COACH: Learning continuous actions from COrrective Advice Communicated by Humans}, 

  year={2015},

  volume={},

  number={},

  pages={581-586},

  doi={10.1109/ICAR.2015.7251514}}
  
% Gaussian COACH

@misc{Gaussian-COACH-wout:2019,
      title={Learning Gaussian Policies from Corrective Human Feedback}, 
      author={Daan Wout and Jan Scholten and Carlos Celemin and Jens Kober},
      year={2019},
      eprint={1903.05216},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

%I-SABL
@article{I-SABL-Loftin:2016,
author = {Loftin, Robert and Peng, Bei and Macglashan, James and Littman, Michael L. and Taylor, Matthew E. and Huang, Jeff and Roberts, David L.},
title = {Learning Behaviors via Human-Delivered Discrete Feedback: Modeling Implicit Feedback Strategies to Speed up Learning},
year = {2016},
issue_date = {January   2016},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {30},
number = {1},
issn = {1387-2532},
url = {https://doi-org.tudelft.idm.oclc.org/10.1007/s10458-015-9283-7},
doi = {10.1007/s10458-015-9283-7},
journal = {Autonomous Agents and Multi-Agent Systems},
month = jan,
pages = {30–59},
numpages = {30},
keywords = {Machine learning, Bayesian inference, Reinforcement learning, Learning from feedback, Interactive learning, Human---computer interaction}
}

%SHIV
@INPROCEEDINGS{SHIV-Laskey:2016,

  author={M. {Laskey} and S. {Staszak} and W. Y. {Hsieh} and J. {Mahler} and F. T. {Pokorny} and A. D. {Dragan} and K. {Goldberg}},

  booktitle={2016 IEEE International Conference on Robotics and Automation (ICRA)}, 

  title={SHIV: Reducing supervisor burden in DAgger using support vectors for efficient learning from demonstrations in high dimensional state spaces}, 

  year={2016},

  volume={},

  number={},

  pages={462-469},

  doi={10.1109/ICRA.2016.7487167}}


%Safe DAGGER
@article{SafeDAgger-Zhang-Cho:2016,
  author    = {Jiakai Zhang and
               Kyunghyun Cho},
  title     = {Query-Efficient Imitation Learning for End-to-End Autonomous Driving},
  journal   = {CoRR},
  volume    = {abs/1605.06450},
  year      = {2016},
  url       = {http://arxiv.org/abs/1605.06450},
  archivePrefix = {arXiv},
  eprint    = {1605.06450},
  timestamp = {Mon, 13 Aug 2018 16:47:18 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/ZhangC16b.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

%Fake COACH
  @misc{fakeCOACH-MacGlashan-Ho-Loftin:2017,
      title={Interactive Learning from Policy-Dependent Human Feedback}, 
      author={James MacGlashan and Mark K Ho and Robert Loftin and Bei Peng and David Roberts and Matthew E. Taylor and Michael L. Littman},
      year={2017},
      eprint={1701.06049},
      archivePrefix={arXiv},
      primaryClass={cs.AI}
}


% AggreVaTeD
@misc{AggreVaTeD-Sun:2017,
      title={Deeply AggreVaTeD: Differentiable Imitation Learning for Sequential Prediction}, 
      author={Wen Sun and Arun Venkatraman and Geoffrey J. Gordon and Byron Boots and J. Andrew Bagnell},
      year={2017},
      eprint={1703.01030},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}
% DART
@misc{DART-Laskey:2017,
      title={DART: Noise Injection for Robust Imitation Learning}, 
      author={M Laskey and Jonathan Lee and Roy Fox and Anca Dragan and Ken Goldberg},
      year={2017},
      eprint={1703.09327},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}
% One-Shot
@misc{One-Shot-Duan:2017,
      title={One-Shot Imitation Learning}, 
      author={Yan Duan and Marcin Andrychowicz and Bradly C. Stadie and Jonathan Ho and Jonas Schneider and Ilya Sutskever and Pieter Abbeel and Wojciech Zaremba},
      year={2017},
      eprint={1703.07326},
      archivePrefix={arXiv},
      primaryClass={cs.AI}
}

% Deep TAMER
  @article{DeepTAMER-Warnell-et-al:2018, title={Deep TAMER: Interactive Agent Shaping in High-Dimensional State Spaces}, volume={32}, url={https://ojs.aaai.org/index.php/AAAI/article/view/11485}, number={1}, journal={Proceedings of the AAAI Conference on Artificial Intelligence}, author={Warnell, Garrett and Waytowich, Nicholas and Lawhern, Vernon and Stone, Peter}, year={2018}, month={Apr.} }

% LOKI
@misc{LOKI-Cheng:2018,
      title={Fast Policy Learning through Imitation and Reinforcement}, 
      author={Ching-An Cheng and Xinyan Yan and Nolan Wagener and Byron Boots},
      year={2018},
      eprint={1805.10413},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

% Hierarchical guidance
@misc{Hierarchical-guidance-Le:2018,
      title={Hierarchical Imitation and Reinforcement Learning}, 
      author={Hoang M. Le and Nan Jiang and Alekh Agarwal and Miroslav Dudík and Yisong Yue and Hal Daumé III au2},
      year={2018},
      eprint={1803.00590},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

% MoBIL
@misc{MoBIL-Cheng:2018,
      title={Accelerating Imitation Learning with Predictive Models}, 
      author={Ching-An Cheng and Xinyan Yan and Evangelos A. Theodorou and Byron Boots},
      year={2018},
      eprint={1806.04642},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}


% D-COACH
@misc{D-COACH-Dattari-Celemin-Ruiz-del-Solar-Kober:2018,
      title={Interactive Learning with Corrective Feedback for Policies based on Deep Neural Networks}, 
      author={Rodrigo Pérez-Dattari and Carlos Celemin and Javier Ruiz-del-Solar and Jens Kober},
      year={2018},
      eprint={1810.00466},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

% BAgger
@inproceedings{BAgger-Cronrath:2018,
  title={BAgger: A Bayesian algorithm for safe and query-efficient imitation learning},
  author={Cronrath, Constantin and Jorge, Emilio and Moberg, John and Jirstrand, Mats and Lennartson, Bengt},year={2018}
}


% DQN-TAMER
  @misc{DQN-TAMER-Arakawa:2018,
      title={DQN-TAMER: Human-in-the-Loop Reinforcement Learning with Intractable Feedback}, 
      author={Riku Arakawa and Sosuke Kobayashi and Yuya Unno and Yuta Tsuboi and Shin-ichi Maeda},
      year={2018},
      eprint={1810.11748},
      archivePrefix={arXiv},
      primaryClass={cs.HC}
}

% SAIL
@article{SAIL-Xiong:2019,
	title={Learning safety-aware policy with imitation learning for context-adaptive navigation},
	author={Xiong, Bo and Wang, Fangshi and Yu, Chao and Liu, XinJun and Qiao, Fei and Yang, Yi and Wei, Qi},
	journal={CEUR Workshop Proceedings},
	year={2019}
}


% HG-DAgger
  @misc{HG-DAgger-Kelly:2019,
      title={HG-DAgger: Interactive Imitation Learning with Human Experts}, 
      author={Michael Kelly and Chelsea Sidrane and Katherine Driggs-Campbell and Mykel J. Kochenderfer},
      year={2019},
      eprint={1810.02890},
      archivePrefix={arXiv},
      primaryClass={cs.RO}
}



% SQIL
@misc{SQIL-Reddy-Dragan-Levine:2019,
      title={SQIL: Imitation Learning via Reinforcement Learning with Sparse Rewards}, 
      author={Siddharth Reddy and Anca D. Dragan and Sergey Levine},
      year={2019},
      eprint={1905.11108},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}
  
% Retrospective DAgger
@misc{Retrospective-DAgger-song:2019,
      title={Learning to Search via Retrospective Imitation}, 
      author={Jialin Song and Ravi Lanka and Albert Zhao and Aadyot Bhatnagar and Yisong Yue and Masahiro Ono},
      year={2019},
      eprint={1804.00846},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

% AOR (Adaptive On-Policy Regularization)
@misc{AOR-lee-laskey:2019,
      title={Dynamic Regret Convergence Analysis and an Adaptive Regularization Algorithm for On-Policy Robot Imitation Learning}, 
      author={Jonathan N. Lee and Michael Laskey and Ajay Kumar Tanwani and Anil Aswani and Ken Goldberg},
      year={2019},
      eprint={1811.02184},
      archivePrefix={arXiv},
      primaryClass={cs.RO}
}

% Dynamic regret - AOR
@inproceedings{Dynamic-regret-Laskey:2018,
  title={Stability Analysis of On-Policy Imitation Learning Algorithms Using Dynamic Regret},
  author={Michael Laskey and A. Tanwani and Ken Goldberg},
  year={2018}
}
% Cycle-of-Learning
@misc{Cycle-of-Learning-waytowich:2018,
      title={Cycle-of-Learning for Autonomous Systems from Human Interaction}, 
      author={Nicholas R. Waytowich and Vinicius G. Goecks and Vernon J. Lawhern},
      year={2018},
      eprint={1808.09572},
      archivePrefix={arXiv},
      primaryClass={cs.AI}
}
% EnsembleDAgger
@misc{EnsembleDAgger-Menda:2019,
      title={EnsembleDAgger: A Bayesian Approach to Safe Imitation Learning}, 
      author={Kunal Menda and Katherine Driggs-Campbell and Mykel J. Kochenderfer},
      year={2019},
      eprint={1807.08364},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

% ValueDICE
@misc{ValueDICE-Kostrikov:2019,
      title={Imitation Learning via Off-Policy Distribution Matching}, 
      author={Ilya Kostrikov and Ofir Nachum and Jonathan Tompson},
      year={2019},
      eprint={1912.05032},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

 %FRESH
 @misc{FRESH-xiao:2020,
      title={FRESH: Interactive Reward Shaping in High-Dimensional State Spaces using Human Feedback}, 
      author={Baicen Xiao and Qifan Lu and Bhaskar Ramasubramanian and Andrew Clark and Linda Bushnell and Radha Poovendran},
      year={2020},
      eprint={2001.06781},
      archivePrefix={arXiv},
      primaryClass={cs.AI}
}
 
% CSF (Converging Supervisor Framework)
@misc{CSF-balakrishna:2020,
      title={On-Policy Robot Imitation Learning from a Converging Supervisor}, 
      author={Ashwin Balakrishna and Brijen Thananjeyan and Jonathan Lee and Felix Li and Arsh Zahed and Joseph E. Gonzalez and Ken Goldberg},
      year={2020},
      eprint={1907.03423},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

% ABLUF
@misc{ABLUF-he:2020,
      title={Learning Behaviors with Uncertain Human Feedback}, 
      author={Xu He and Haipeng Chen and Bo An},
      year={2020},
      eprint={2006.04201},
      archivePrefix={arXiv},
      primaryClass={cs.AI}
}
% DA-RB (DAgger Replay Buffer)
@INPROCEEDINGS{DA-RB-Prakash:2020,

  author={A. {Prakash} and A. {Behl} and E. {Ohn-Bar} and K. {Chitta} and A. {Geiger}},

  booktitle={2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)}, 

  title={Exploring Data Aggregation in Policy Learning for Vision-Based Urban Autonomous Driving}, 

  year={2020},

  volume={},

  number={},

  pages={11760-11770},

  doi={10.1109/CVPR42600.2020.01178}}


% APIL
@misc{APIL-nguyen:2020,
      title={Active Imitation Learning from Multiple Non-Deterministic Teachers: Formulation, Challenges, and Algorithms}, 
      author={Khanh Nguyen and Hal Daumé III au2},
      year={2020},
      eprint={2006.07777},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

% Active Imitation Learnng (related to APIL)
@article{Active-Imitation-Learning-judah:2014,
  author  = {Kshitij Judah and Alan P. Fern and Thomas G. Dietterich and Prasad Tadepalli},
  title   = {Active Imitation Learning: Formal and Practical Reductions to I.I.D. Learning},
  journal = {Journal of Machine Learning Research},
  year    = {2014},
  volume  = {15},
  number  = {120},
  pages   = {4105-4143},
  url     = {http://jmlr.org/papers/v15/judah14a.html}
}

% EIL
@inproceedings{EIL-Spencer:2020,
author = {Spencer, Jonathan and Choudhury, Sanjiban and Barnes, Matt and Schmittle, Matthew and Chiang, Mung and Ramadge, Peter and Srinivasa, Siddhartha},
year = {2020},
month = {07},
pages = {},
title = {Learning from Interventions: Human-robot interaction as both explicit and implicit feedback},
doi = {10.15607/RSS.2020.XVI.055}
}

% FIRE
@misc{FIRE-ablett:2020,
      title={Fighting Failures with FIRE: Failure Identification to Reduce Expert Burden in Intervention-Based Learning}, 
      author={Trevor Ablett and Filip Marić and Jonathan Kelly},
      year={2020},
      eprint={2007.00245},
      archivePrefix={arXiv},
      primaryClass={cs.RO}
}

% IWR
  
@misc{IWR-mandlekar:2020,
      title={Human-in-the-Loop Imitation Learning using Remote Teleoperation}, 
      author={Ajay Mandlekar and Danfei Xu and Roberto Martín-Martín and Yuke Zhu and Li Fei-Fei and Silvio Savarese},
      year={2020},
      eprint={2012.06733},
      archivePrefix={arXiv},
      primaryClass={cs.RO}
}

% Neural Networks
@book{ANN-graupe:2013,
  title={Principles Of Artificial Neural Networks (3rd Edition)},
  author={Graupe, D.},
  isbn={9789814522755},
  series={Advanced Series In Circuits And Systems},
  url={https://books.google.es/books?id=Zz27CgAAQBAJ},
  year={2013},
  publisher={World Scientific Publishing Company}
}

% Convolutional Tikz
  
@misc{Convolutional-tikz,
      title={Drawing a convolution with Tikz}, 
      author={Herman Jaramillo},
      year={2019},
      url= {https://tex.stackexchange.com/a/579557}
}

% Neural Network Tikz
  
@misc{NN-tikz,
      title={Diagram of an artificial neural network}, 
      author={Gonzalo Medina},
      year={2013},
      url= {https://tex.stackexchange.com/a/132471}
}


% Autoencoder Tikz
  
@misc{Autoencoder-tikz,
      title={LaTeX-TikZ-Diagrams}, 
      author={James Allingham},
      year={2020},
      url= {https://github.com/JamesAllingham/LaTeX-TikZ-Diagrams}
}

% Vanilla RNN
  
@misc{Vanilla-RNN-image,
      title={Creating A Text Generator Using Recurrent Neural Network }, 
      author={Trung Tran},
      year={2020},
      url= {https://chunml.github.io/ChunML.github.io/project/Creating-Text-Generator-Using-Recurrent-Neural-Network/}
}

% LazyDagger for tree of IL > IIL
@misc{lazydagger:2021,
      title={LazyDAgger: Reducing Context Switching in Interactive Imitation Learning}, 
      author={Ryan Hoque and Ashwin Balakrishna and Carl Putterman and Michael Luo and Daniel S. Brown and Daniel Seita and Brijen Thananjeyan and Ellen Novoseller and Ken Goldberg},
      year={2021},
      eprint={2104.00053},
      archivePrefix={arXiv},
      primaryClass={cs.RO}
}


% Another Laskey definition of on-policy off-policy IL
  
  @misc{OtherLaskeydefinitions:2019,
      title={Dynamic Regret Convergence Analysis and an Adaptive Regularization Algorithm for On-Policy Robot Imitation Learning}, 
      author={Jonathan N. Lee and Michael Laskey and Ajay Kumar Tanwani and Anil Aswani and Ken Goldberg},
      year={2019},
      eprint={1811.02184},
      archivePrefix={arXiv},
      primaryClass={cs.RO}
}
  
  % Another definition of on-policy off-policy IL from Berkeley
  @misc{Anotherdefinitionfromberkeley:2020,
      title={On-Policy Robot Imitation Learning from a Converging Supervisor}, 
      author={Ashwin Balakrishna and Brijen Thananjeyan and Jonathan Lee and Felix Li and Arsh Zahed and Joseph E. Gonzalez and Ken Goldberg},
      year={2020},
      eprint={1907.03423},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

% Temporal Difference equation

@article{TD:equation:2019,
  title={Reinforcement Learning: Algorithms and Convergence},
  author={Heitzinger, Clemens},
  year={2019}
}

% Problem of collecting data
@ARTICLE{collecting_data_for_offline_learning,
  author={Wu, Qingyao and Wu, Hanrui and Zhou, Xiaoming and Tan, Mingkui and Xu, Yonghui and Yan, Yuguang and Hao, Tianyong},

  journal={IEEE Transactions on Knowledge and Data Engineering}, 
  title={Online Transfer Learning with Multiple Homogeneous or Heterogeneous Sources}, 
  year={2017},
  volume={29},
  number={7},
  pages={1494-1507},
  doi={10.1109/TKDE.2017.2685597}}
  
  
  % Artificial intelligence book
  @book{artificial_intelligence_book,
  title={Artificial Intelligence: Foundations of Computational Agents},
  author={Poole, D.L. and Mackworth, A.K.},
  isbn={9781108171021},
  url={https://books.google.nl/books?id=q6t2DwAAQBAJ},
  year={2017},
  publisher={Cambridge University Press}
}

  
% Experience replay catastrophic forgetting
@misc{catastrophic_forgetting,
      title={Memory Efficient Experience Replay for Streaming Learning}, 
      author={Tyler L. Hayes and Nathan D. Cahill and Christopher Kanan},
      year={2019},
      eprint={1809.05922},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

% Experience Replay provides stability
@misc{Experience_replay_stability:2019,
      title={Experience Replay for Continual Learning}, 
      author={David Rolnick and Arun Ahuja and Jonathan Schwarz and Timothy P. Lillicrap and Greg Wayne},
      year={2019},
      eprint={1811.11682},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

% Machine Learning for finance

@book{Machine_learning_finance:2019,
  title={Machine Learning for Finance: Principles and practice for financial insiders},
  author={Klaas, J.},
  isbn={9781789134698},
  url={https://books.google.nl/books?id=qfeaDwAAQBAJ},
  year={2019},
  publisher={Packt Publishing}
}