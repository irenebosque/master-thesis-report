%%%%%%%%%%%%%%%%%%%%%%%%% INSTRUCTIONS %%%%%%%%%%%%%%%%%%%%%%%%%

% Below you will be able to find a few examples for the most common types of references (articles, books and websites). Please ensure the reference follows the same format if you are writing it yourself. However, especially with articles, you might be better off finding the 'cite this reference' button on the website. 

% You can cite using \cite{name-of-entry}. The 'name-of-entry' is the part after the first line. For this for example below, citing would work like: \cite{Example-Article}. Once you have cited the source at least once in the text, the entry will appear in the bibliography at the end.

% For a cheat sheet with all entry types and much more, see http://tug.ctan.org/info/biblatex-cheatsheet/biblatex-cheatsheet.pdf

% Intorduction: Need of flexible approaches
@INPROCEEDINGS{need-of-flexible-control-approaches,

  author={L. {Nardi} and C. {Stachniss}},

  booktitle={2016 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)}, 

  title={Experience-based path planning for mobile robots exploiting user preferences}, 

  year={2016},

  volume={},

  number={},

  pages={1170-1176},

  doi={10.1109/IROS.2016.7759197}}
  

% openAI hand




@article{openAI-hand,
author = {OpenAI: Marcin Andrychowicz and Bowen Baker and Maciek Chociej and Rafal Józefowicz and Bob McGrew and Jakub Pachocki and Arthur Petron and Matthias Plappert and Glenn Powell and Alex Ray and Jonas Schneider and Szymon Sidor and Josh Tobin and Peter Welinder and Lilian Weng and Wojciech Zaremba},
title ={Learning dexterous in-hand manipulation},
journal = {The International Journal of Robotics Research},
volume = {39},
number = {1},
pages = {3-20},
year = {2020},
doi = {10.1177/0278364919887447},

URL = { 
        https://doi.org/10.1177/0278364919887447
    
},
eprint = { 
        https://doi.org/10.1177/0278364919887447
    
}
,
    abstract = { We use reinforcement learning (RL) to learn dexterous in-hand manipulation policies that can perform vision-based object reorientation on a physical Shadow Dexterous Hand. The training is performed in a simulated environment in which we randomize many of the physical properties of the system such as friction coefficients and an object’s appearance. Our policies transfer to the physical robot despite being trained entirely in simulation. Our method does not rely on any human demonstrations, but many behaviors found in human manipulation emerge naturally, including finger gaiting, multi-finger coordination, and the controlled use of gravity. Our results were obtained using the same distributed RL system that was used to train OpenAI Five. We also include a video of our results: https://youtu.be/jwSbzNHGflM. }
}



% alphaGO

@article{alphaGO-silver-2016,
  author = {Silver, David and Huang, Aja and Maddison, Chris J. and Guez, Arthur and Sifre, Laurent and van den Driessche, George and Schrittwieser, Julian and Antonoglou, Ioannis and Panneershelvam, Veda and Lanctot, Marc and Dieleman, Sander and Grewe, Dominik and Nham, John and Kalchbrenner, Nal and Sutskever, Ilya and Lillicrap, Timothy and Leach, Madeleine and Kavukcuoglu, Koray and Graepel, Thore and Hassabis, Demis},
  biburl = {https://www.bibsonomy.org/bibtex/29e987f58d895c490144693139cbc90c7/flint63},
  doi = {10.1038/nature16961},
  file = {Nature online:2016/SilverHuangEtAl16nature.pdf:PDF},
  groups = {public},
  interhash = {48430c7891aaf9fe2582faa8f5d076c1},
  intrahash = {9e987f58d895c490144693139cbc90c7},
  issn = {0028-0836},
  journal = {Nature},
  keywords = {01614 paper ai google learn algorithm},
  month = jan,
  number = 7587,
  pages = {484--489},
  timestamp = {2018-04-16T12:03:12.000+0200},
  title = {Mastering the Game of {Go} with Deep Neural Networks and Tree Search},
  username = {flint63},
  volume = 529,
  year = 2016
}

% % ATARI RL
% @misc{Atari-RL,
%       title={Playing Atari with Deep Reinforcement Learning}, 
%       author={Volodymyr Mnih and Koray Kavukcuoglu and David Silver and Alex Graves and Ioannis Antonoglou and Daan Wierstra and Martin Riedmiller},
%       year={2013},
%       eprint={1312.5602},
%       archivePrefix={arXiv},
%       primaryClass={cs.LG}
% }


@article{Atari-RL,
  abstract = {We present the first deep learning model to successfully learn control
policies directly from high-dimensional sensory input using reinforcement
learning. The model is a convolutional neural network, trained with a variant
of Q-learning, whose input is raw pixels and whose output is a value function
estimating future rewards. We apply our method to seven Atari 2600 games from
the Arcade Learning Environment, with no adjustment of the architecture or
learning algorithm. We find that it outperforms all previous approaches on six
of the games and surpasses a human expert on three of them.},
  added-at = {2019-07-12T20:11:01.000+0200},
  author = {Mnih, Volodymyr and Kavukcuoglu, Koray and Silver, David and Graves, Alex and Antonoglou, Ioannis and Wierstra, Daan and Riedmiller, Martin},
  biburl = {https://www.bibsonomy.org/bibtex/2a00ec4c09f5dc9b3f8a1836f4e02bb5d/lanteunis},
  interhash = {78966703f649bae69a08a6a23a4e8879},
  intrahash = {a00ec4c09f5dc9b3f8a1836f4e02bb5d},
  keywords = {},
  note = {NIPS Deep Learning Workshop 2013},
  timestamp = {2019-07-12T20:11:01.000+0200},
  title = {Playing Atari with Deep Reinforcement Learning},
  url = {http://arxiv.org/abs/1312.5602},
  year = 2013
}






@article{reinforcement-learning-costly-Kober:2013,
author = {Jens Kober and J. Andrew Bagnell and Jan Peters},
title ={Reinforcement learning in robotics: A survey},
journal = {The International Journal of Robotics Research},
volume = {32},
number = {11},
pages = {1238-1274},
year = {2013},
doi = {10.1177/0278364913495721}



}

% Survey on types of feedback
% @misc{types-feedback-najar:2020,
%       title={Reinforcement learning with human advice: a survey}, 
%       author={Anis Najar and Mohamed Chetouani},
%       year={2020},
%       eprint={2005.11016},
%       archivePrefix={arXiv},
%       primaryClass={cs.AI}
% }




@ARTICLE{types-feedback-najar:2020,
  
AUTHOR={Najar, Anis and Chetouani, Mohamed},   
	 
TITLE={Reinforcement Learning With Human Advice: A Survey},      
	
JOURNAL={Frontiers in Robotics and AI},      
	
VOLUME={8},      

PAGES={74},     
	
YEAR={2021},      
	  
URL={https://www.frontiersin.org/article/10.3389/frobt.2021.584075},       
	
  
	
ISSN={2296-9144},   
   
ABSTRACT={In this paper, we provide an overview of the existing methods for integrating human advice into a reinforcement learning process. We first propose a taxonomy of the different forms of advice that can be provided to a learning agent. We then describe the methods that can be used for interpreting advice when its meaning is not determined beforehand. Finally, we review different approaches for integrating advice into the learning process.}
}



% Relative corrections Celemin
@article{Relative-corrections-Celemin:2019,
author = {Celemin, Carlos and Ruiz-Del-Solar, Javier},
title = {An Interactive Framework for Learning Continuous Actions Policies Based on Corrective Feedback},
year = {2019},
issue_date = {July      2019},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {95},
number = {1},
issn = {0921-0296},
url = {https://doi-org.tudelft.idm.oclc.org/10.1007/s10846-018-0839-z},
journal = {J. Intell. Robotics Syst.},
month = jul,
pages = {77–97},
numpages = {21},
keywords = {Interactive machine learning, Learning from demonstration, Human feedback, Human teachers, Decision making systems}
}

  




% Sergey Levine Youtube
@online{youtube_offline_RL,
    title = {Offline Reinforcement Learning},
    year = {2020},
    organization = {University of Berkeley},
    author = {Sergey Levine},
    url = {https://www.youtube.com/watch?v=qgZPZREor5I},
    }
% Offline RL - Paper de Sergey Levine

@article{Offline-RL-Levine:2020,
  title={Offline reinforcement learning: Tutorial, review, and perspectives on open problems},
  author={Levine, Sergey and Kumar, Aviral and Tucker, George and Fu, Justin},
  journal={arXiv preprint arXiv:2005.01643},
  year={2020}
}


% Offline RL - Google
@online{Google_offline_RL,
    title = {An Optimistic Perspective on Offline Reinforcement Learning },
    year = {2020},
    organization = {Google Research},
    author = {Rishabh Agarwal},
    url = {https://ai.googleblog.com/2020/04/an-optimistic-perspective-on-offline.html},
    }

% A Deeper Look at Experience Replay
@article{Experience-Replay-zhang:2018,
      title={A Deeper Look at Experience Replay}, 
      author={Shangtong Zhang and Richard S. Sutton},
      year={2018},
    
      journal={arXiv preprint:1712.01275}
}


[arXiv preprint \href{http://arxiv.org/abs/1409.0876}{arXiv:1409.0876}, September 2014]

% Neural Networks classification
@book{Classification-Artificial-Neural-Networks:2017,
  title={Neural Networks with R: Smart models using CNN, RNN, deep learning, and artificial intelligence principles},
  author={Ciaburro, G. and Venkateswaran, B.},
  isbn={9781788399418},
  url={https://books.google.es/books?id=IppGDwAAQBAJ},
  year={2017},
  publisher={Packt Publishing}
}
% Global overview of Imitation Learning


@article{Global-overview-Attia:2018,
  title={Global overview of imitation learning},
  author={Attia, Alexandre and Dayan, Sharone},
  journal={arXiv preprint:1801.06503},
  year={2018}
}


% Leveraging Human Guidance for Deep Reinforcement Learning Tasks



% @article{leveraging-human-guidance:2019,
%   author    = {Ruohan Zhang and
%               Faraz Torabi and
%               Lin Guan and
%               Dana H. Ballard and
%               Peter Stone},
%   title     = {Leveraging Human Guidance for Deep Reinforcement Learning Tasks},
%   journal   = {CoRR},
%   volume    = {abs/1909.09906},
%   year      = {2019},
%   url       = {http://arxiv.org/abs/1909.09906},
%   archivePrefix = {arXiv},
%   eprint    = {1909.09906},
%   timestamp = {Fri, 27 Sep 2019 13:04:21 +0200},
%   biburl    = {https://dblp.org/rec/journals/corr/abs-1909-09906.bib},
%   bibsource = {dblp computer science bibliography, https://dblp.org}
% }


@inproceedings{leveraging-human-guidance:2019,
  title     = {Leveraging Human Guidance for Deep Reinforcement Learning Tasks},
  author    = {Zhang, Ruohan and Torabi, Faraz and Guan, Lin and Ballard, Dana H. and Stone, Peter},
  booktitle = {Proceedings of the Twenty-Eighth International Joint Conference on
               Artificial Intelligence, {IJCAI-19}},
  publisher = {International Joint Conferences on Artificial Intelligence Organization},             
  pages     = {6339--6346},
  year      = {2019},
  month     = {7},

  url       = {https://doi.org/10.24963/ijcai.2019/884},
}


% Why sometimes is easier learning a policy than learning a reward
@article{kostrikov2019imitation,
      title={Imitation Learning via Off-Policy Distribution Matching}, 
      author={Ilya Kostrikov and Ofir Nachum and Jonathan Tompson},
      year={2019},

    
       journal={arXiv preprint:1912.05032},
      
}


% Original Q-learning paper
@PhdThesis{Watkins:1989,
  author =       "Watkins, Christopher John Cornish Hellaby",
  title =        "Learning from Delayed Rewards",
  school =       "King's College",
  year =         "1989",
  address =   "Cambridge, UK",
  month =     {05},
  url = "http://www.cs.rhul.ac.uk/~chrisw/new_thesis.pdf",
  bib2html_rescat = "Parameter",
}
% Original SARSA paper
@TechReport{Rummery+Niranjan:1994,
  author =       "Rummery, G. A. and Niranjan, M.",
  title =        "On-line {Q}-learning using connectionist systems",
  institution =  "Cambridge University Engineering Department",
  year =         "1994",
  type =      "CUED/F-INFENG/TR",
  number =    "166",
  month =     {09},
  bibdate =      "Thu Feb 10 16:53:24 1994",
  url = "ftp://svr-ftp.eng.cam.ac.uk/reports/rummery_tr166.ps.Z",
  bib2html_rescat = "Learning Methods, General RL",
}

% Book An Algorithmic Perspective on Imitation Learning where they say when was the first mention to off-policy IL

@BOOK{Osa:2018,  author={T. {Osa} and J. {Pajarinen} and G. {Neumann} and J. A. {Bagnell} and P. {Abbeel} and J. {Peters}},  title={An Algorithmic Perspective on Imitation Learning},  year={2018},  volume={},  number={},  pages={},  doi={10.1561/2300000053}}


% FIrst mention of On-policy Off-policy in IMitation Learning
% @article{DBLP:journals/corr/LaskeyLHLMFG17,
%   author    = {Michael Laskey and
%               Jonathan Lee and
%               Wesley Yu{-}Shu Hsieh and
%               Richard Liaw and
%               Jeffrey Mahler and
%               Roy Fox and
%               Ken Goldberg},
%   title     = {Iterative Noise Injection for Scalable Imitation Learning},
%   journal   = {CoRR},
%   volume    = {abs/1703.09327},
%   year      = {2017},
%   url       = {http://arxiv.org/abs/1703.09327v1},
%   archivePrefix = {arXiv},
%   eprint    = {1703.09327},
%   timestamp = {Mon, 23 Nov 2020 08:36:40 +0100},
%   biburl    = {https://dblp.org/rec/journals/corr/LaskeyLHLMFG17.bib},
%   bibsource = {dblp computer science bibliography, https://dblp.org}
% }


@article{DBLP:journals/corr/LaskeyLHLMFG17,
      title={Iterative Noise Injection for Scalable Imitation Learning}, 
      author={Michael Laskey and
               Jonathan Lee and
               Wesley Yu{-}Shu Hsieh and
               Richard Liaw and
               Jeffrey Mahler and
               Roy Fox and
               Ken Goldberg},
      year={2017},

    
       journal={arXiv preprint:1703.09327v1},
      
}

% Biblia del RL

@book{Sutton:1998,
author = {Sutton, Richard S. and Barto, Andrew G.},
title = {Introduction to Reinforcement Learning},
year = {1998},
isbn = {0262193981},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
edition = {1st},
abstract = {From the Publisher:In Reinforcement Learning, Richard Sutton and Andrew Barto provide a clear and simple account of the key ideas and algorithms of reinforcement learning. Their discussion ranges from the history of the field's intellectual foundations to the most recent developments and applications. The only necessary mathematical background is familiarity with elementary concepts of probability.}
}

  


% Human-centric sampling vs Robot-centric sampling

@misc{laskey2017comparing,
      title={Comparing Human-Centric and Robot-Centric Sampling for Robot Deep Learning from Demonstrations}, 
      author={Michael Laskey and Caleb Chuck and Jonathan Lee and Jeffrey Mahler and Sanjay Krishnan and Kevin Jamieson and Anca Dragan and Ken Goldberg},
      year={2017},
      eprint={1610.00850},
      archivePrefix={arXiv},
      primaryClass={cs.RO}
}

% Laskey phd thesis
@phdthesis{Laskey:phdthesis,
    Author = {Laskey, Michael},
    Title = {On and Off-Policy Deep Imitation Learning for Robotics},
    School = {EECS Department, University of California, Berkeley},
    Year = {2018},
    Month = {08},
    URL = {http://www2.eecs.berkeley.edu/Pubs/TechRpts/2018/EECS-2018-108.html},
    Number = {UCB/EECS-2018-108},
    Abstract = {As an alternative to explicit programming for robots, Deep Imitation learning has two drawbacks: sample complexity and covariate shift.  One approach to Imitation Learning is Behavior Cloning, in which a
robot observes a supervisor and then infers a control policy. A known problem with this approach is that even slight departures from the supervisor’s demonstrations can compound over the policy’s roll-out resulting in errors; this concept of drift and resulting error is commonly referred to as covariate shift On-policy techniques reduce covariate shift by iteratively collecting corrective actions for the current robot policy.  To reduce sample complexity of these approaches, we propose a novel active learning algorithm, SHIV (Svm-based reduction in Human InterVention). While evaluating SHIV, we reconsider the trade-off between Off- and On-Policy methods and find that: 1) On-Policy methods are challenging for humans supervisors and 2) performance varies with the expressiveness of the policy class. To make Off-Policy methods more robust for expressive policies we propose a second algorithm, DART (Disturbances Augmenting Robot Trajectories), which injects optimized noise into the supervisor’s control stream to simulate error during data collection. This dissertation contributes two aforementioned algorithms, experimental evaluation with three robots evaluating their performance on tasks ranging from grasping in clutter to singulation to bed-making, and the design of a novel first-order urban driving simulator (FLUIDS) that can fill gaps in existing benchmarks for Imitation Learning to rapidly test algorithm performance in terms of generalization.}
}

% Research Assignment paper
@ARTICLE{ResearchAssignmentpaper,

  author={R. {Perez-Dattari} and C. {Celemin} and G. {Franzese} and J. {Ruiz-del-Solar} and J. {Kober}},

  journal={IEEE Robotics   Automation Magazine}, 

  title={Interactive Learning of Temporal Features for Control: Shaping Policies and State Representations From Human Feedback}, 

  year={2020},

  volume={27},

  number={2},

  pages={46-54},

  doi={10.1109/MRA.2020.2983649}}
  
  
  % Imition Learning definition
%   @misc{Imitation-Learning-definition-torabi:2019,
%       title={Recent Advances in Imitation Learning from Observation}, 
%       author={Faraz Torabi and Garrett Warnell and Peter Stone},
%       year={2019},
%       eprint={1905.13566},
%       archivePrefix={arXiv},
%       primaryClass={cs.RO}
% }



@inproceedings{Imitation-Learning-definition-torabi:2019,
  title     = {Recent Advances in Imitation Learning from Observation},
  author    = {Torabi, Faraz and Warnell, Garrett and Stone, Peter},
  booktitle = {Proceedings of the Twenty-Eighth International Joint Conference on
               Artificial Intelligence, {IJCAI-19}},
  publisher = {International Joint Conferences on Artificial Intelligence Organization},             
  pages     = {6325--6331},
  year      = {2019},
  month     = {7},
  url       = {https://doi.org/10.24963/ijcai.2019/882},
}
  
  
  % Behavioural Cloning - 1991
  @ARTICLE{Behavioural-Cloning-Pomerleau:1991,

  author={D. A. {Pomerleau}},

  journal={Neural Computation}, 

  title={Efficient Training of Artificial Neural Networks for Autonomous Navigation}, 

  year={1991},

  volume={3},

  number={1},

  pages={88-97},

  doi={10.1162/neco.1991.3.1.88}}

% SEARN - 2009
@misc{SEARN-DaumeIII:2009,
      title={Search-based Structured Prediction}, 
      author={Hal DauméIII and John Langford and Daniel Marcu},
      year={2009},
      eprint={0907.0786},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}
  
  % TAMER
%   @InProceedings{TAMER-Knox-Stone:2009,
% title={Interactively Shaping Agents via Human Reinforcement: The TAMER Framework},
% author={W. Bradley Knox and Peter Stone},
% booktitle={The Fifth International Conference on Knowledge Capture},
% month={09},
% url="http://www.cs.utexas.edu/users/ai-lab?KCAP09-knox",
% year={2009}
% }


@inproceedings{TAMER-Knox-Stone:2009,
author = {Knox, W. Bradley and Stone, Peter},
title = {Interactively Shaping Agents via Human Reinforcement: The TAMER Framework},
year = {2009},
isbn = {9781605586588},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},

doi = {10.1145/1597735.1597738},
abstract = {As computational learning agents move into domains that incur real costs (e.g., autonomous driving or financial investment), it will be necessary to learn good policies without numerous high-cost learning trials. One promising approach to reducing sample complexity of learning a task is knowledge transfer from humans to agents. Ideally, methods of transfer should be accessible to anyone with task knowledge, regardless of that person's expertise in programming and AI. This paper focuses on allowing a human trainer to interactively shape an agent's policy via reinforcement signals. Specifically, the paper introduces "Training an Agent Manually via Evaluative Reinforcement," or TAMER, a framework that enables such shaping. Differing from previous approaches to interactive shaping, a TAMER agent models the human's reinforcement and exploits its model by choosing actions expected to be most highly reinforced. Results from two domains demonstrate that lay users can train TAMER agents without defining an environmental reward function (as in an MDP) and indicate that human training within the TAMER framework can reduce sample complexity over autonomous learning algorithms.},
booktitle = {Proceedings of the Fifth International Conference on Knowledge Capture},
pages = {9–16},
numpages = {8},
keywords = {sequential decision-making, shaping, human teachers, human-agent interaction, learning agents},
location = {Redondo Beach, California, USA},
series = {K-CAP '09}
}

% SMILe
 @InProceedings{SMILe-Ross-Bagnell:2010, title = {Efficient Reductions for Imitation Learning}, author = {Stephane Ross and Drew Bagnell}, booktitle = {Proceedings of the Thirteenth International Conference on Artificial Intelligence and Statistics}, pages = {661--668}, year = {2010}, editor = {Yee Whye Teh and Mike Titterington}, volume = {9}, series = {Proceedings of Machine Learning Research}, address = {Chia Laguna Resort, Sardinia, Italy}, month = {13--15 May}, publisher = {JMLR Workshop and Conference Proceedings}, pdf = {http://proceedings.mlr.press/v9/ross10a/ross10a.pdf}, url = {http://proceedings.mlr.press/v9/ross10a.html}} 
 
 % DAgger

@inproceedings{DAgger-Ross:2011,
  title={A reduction of imitation learning and structured prediction to no-regret online learning},
  author={Ross, St{\'e}phane and Gordon, Geoffrey and Bagnell, Drew},
  booktitle={Proceedings of the fourteenth international conference on artificial intelligence and statistics},
  pages={627--635},
  year={2011},
  organization={JMLR Workshop and Conference Proceedings}
}

% DAgger by coaching
@article{DAgger-by-coaching-He-DaumeIII-Eisner:2012,
author = {He, H. and III, H. and Eisner, J.},
year = {2012},
month = {01},
pages = {3149-3157},
title = {Imitation learning by coaching},
volume = {4},
journal = {Advances in Neural Information Processing Systems}
}

%Advise
@article{Advise-Griffith-et-al:2013,
author = {Griffith, S. and Subramanian, K. and Scholz, Jonathan and Isbell, C.L. and Thomaz, A.L.},
year = {2013},
month = {01},
pages = {},
title = {Policy shaping: Integrating human feedback with Reinforcement Learning},
journal = {Advances in Neural Information Processing Systems}
}

%AggreVaTe


@article{AggreVaTe-Ross-Bagnell:2014,
  title={Reinforcement and imitation learning via interactive no-regret learning},
  author={Ross, Stephane and Bagnell, J Andrew},
  journal={arXiv preprint:1406.5979},
  year={2014}
}



% COACH

@INPROCEEDINGS{COACH-Celemin-Ruiz-del-Solar:2015,

  author={C. {Celemin} and J. {Ruiz-del-Solar}},

  booktitle={2015 International Conference on Advanced Robotics (ICAR)}, 

  title={COACH: Learning continuous actions from COrrective Advice Communicated by Humans}, 

  year={2015},

  volume={},

  number={},

  pages={581-586},

  doi={10.1109/ICAR.2015.7251514}}
  
% Gaussian COACH


@article{Gaussian-COACH-wout:2019,
  title={Learning Gaussian policies from corrective human feedback},
  author={Wout, Daan and Scholten, Jan and Celemin, Carlos and Kober, Jens},
  journal={arXiv preprint:1903.05216},
  year={2019}
}


%I-SABL
@article{I-SABL-Loftin:2016,
author = {Loftin, Robert and Peng, Bei and Macglashan, James and Littman, Michael L. and Taylor, Matthew E. and Huang, Jeff and Roberts, David L.},
title = {Learning Behaviors via Human-Delivered Discrete Feedback: Modeling Implicit Feedback Strategies to Speed up Learning},
year = {2016},
issue_date = {January   2016},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {30},
number = {1},
issn = {1387-2532},
doi = {10.1007/s10458-015-9283-7},
journal = {Autonomous Agents and Multi-Agent Systems},
month = jan,
pages = {30–59},
numpages = {30},
keywords = {Machine learning, Bayesian inference, Reinforcement learning, Learning from feedback, Interactive learning, Human---computer interaction}
}

%SHIV
@INPROCEEDINGS{SHIV-Laskey:2016,

  author={M. {Laskey} and S. {Staszak} and W. Y. {Hsieh} and J. {Mahler} and F. T. {Pokorny} and A. D. {Dragan} and K. {Goldberg}},

  booktitle={2016 IEEE International Conference on Robotics and Automation (ICRA)}, 

  title={SHIV: Reducing supervisor burden in DAgger using support vectors for efficient learning from demonstrations in high dimensional state spaces}, 

  year={2016},

  volume={},

  number={},

  pages={462-469},

  doi={10.1109/ICRA.2016.7487167}}


%Safe DAGGER

@inproceedings{SafeDAgger-Zhang-Cho:2016,
  title={Query-efficient imitation learning for end-to-end simulated driving},
  author={Zhang, Jiakai and Cho, Kyunghyun},
  booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},
  volume={31},
  number={1},
  year={2017}
}

%Fake COACH

@inproceedings{fakeCOACH-MacGlashan-Ho-Loftin:2017,
  title={Interactive learning from policy-dependent human feedback},
  author={MacGlashan, James and Ho, Mark K and Loftin, Robert and Peng, Bei and Wang, Guan and Roberts, David L and Taylor, Matthew E and Littman, Michael L},
  booktitle={International Conference on Machine Learning},
  pages={2285--2294},
  year={2017},
  organization={PMLR}
}


% AggreVaTeD

@inproceedings{AggreVaTeD-Sun:2017,
  title={Deeply aggrevated: Differentiable imitation learning for sequential prediction},
  author={Sun, Wen and Venkatraman, Arun and Gordon, Geoffrey J and Boots, Byron and Bagnell, J Andrew},
  booktitle={International Conference on Machine Learning},
  pages={3309--3318},
  year={2017},
  organization={PMLR}
}

% DART
% @misc{DART-Laskey:2017,
%       title={DART: Noise Injection for Robust Imitation Learning}, 
%       author={M Laskey and Jonathan Lee and Roy Fox and Anca Dragan and Ken Goldberg},
%       year={2017},
%       eprint={1703.09327},
%       archivePrefix={arXiv},
%       primaryClass={cs.LG}
% }



@inproceedings{DART-Laskey:2017,
  author    = {Michael Laskey and
               Jonathan Lee and
               Roy Fox and
               Anca D. Dragan and
               Ken Goldberg},
  title     = {{DART:} Noise Injection for Robust Imitation Learning},
  booktitle = {1st Annual Conference on Robot Learning, CoRL 2017},
  volume    = {78},
  pages     = {143--156},

  year      = {2017},
  url       = {http://proceedings.mlr.press/v78/laskey17a.html},
  timestamp = {Mon, 23 Nov 2020 08:36:59 +0100},
  biburl    = {https://dblp.org/rec/conf/corl/LaskeyLFDG17.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}
% One-Shot
@misc{One-Shot-Duan:2017,
      title={One-Shot Imitation Learning}, 
      author={Yan Duan and Marcin Andrychowicz and Bradly C. Stadie and Jonathan Ho and Jonas Schneider and Ilya Sutskever and Pieter Abbeel and Wojciech Zaremba},
      year={2017},
      eprint={1703.07326},
      archivePrefix={arXiv},
      primaryClass={cs.AI}
}

% Deep TAMER
  @article{DeepTAMER-Warnell-et-al:2018, title={Deep TAMER: Interactive Agent Shaping in High-Dimensional State Spaces}, volume={32}, url={https://ojs.aaai.org/index.php/AAAI/article/view/11485}, number={1}, journal={Proceedings of the AAAI Conference on Artificial Intelligence}, author={Warnell, Garrett and Waytowich, Nicholas and Lawhern, Vernon and Stone, Peter}, year={2018}, month={04} }

% LOKI

@inproceedings{LOKI-Cheng:2018,
  author    = {Ching{-}An Cheng and
               Xinyan Yan and
               Nolan Wagener and
               Byron Boots},
  title     = {Fast Policy Learning through Imitation and Reinforcement},
  booktitle = {Proceedings of the Thirty-Fourth Conference on Uncertainty in Artificial
               Intelligence},
  pages     = {845--855},
  publisher = {{AUAI} Press},
  year      = {2018},
  url       = {http://auai.org/uai2018/proceedings/papers/302.pdf},
  timestamp = {Tue, 15 Dec 2020 17:40:18 +0100},
  biburl    = {https://dblp.org/rec/conf/uai/ChengYWB18.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

% Hierarchical guidance

@inproceedings{Hierarchical-guidance-Le:2018,
  title={Hierarchical imitation and reinforcement learning},
  author={Le, Hoang and Jiang, Nan and Agarwal, Alekh and Dud{\'\i}k, Miroslav and Yue, Yisong and Daum{\'e}, Hal},
  booktitle={International conference on machine learning},
  pages={2917--2926},
  year={2018},
  organization={PMLR}
}


% MoBIL
@misc{MoBIL-Cheng:2018,
      title={Accelerating Imitation Learning with Predictive Models}, 
      author={Ching-An Cheng and Xinyan Yan and Evangelos A. Theodorou and Byron Boots},
      year={2018},
      eprint={1806.04642},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}


% D-COACH

@inproceedings{D-COACH-Dattari-Celemin-Ruiz-del-Solar-Kober:2018,
  title={Interactive learning with corrective feedback for policies based on deep neural networks},
  author={P{\'e}rez-Dattari, Rodrigo and Celemin, Carlos and Ruiz-del-Solar, Javier and Kober, Jens},
  booktitle={International Symposium on Experimental Robotics},
  pages={353--363},
  year={2018},
  organization={Springer}
}


% BAgger

@inproceedings{BAgger-Cronrath:2018,
  title={BAgger: A Bayesian algorithm for safe and query-efficient imitation learning},
  author={Cronrath, Constantin and Jorge, Emilio and Moberg, John and Jirstrand, Mats and Lennartson, Bengt},
  booktitle={Machine Learning in Robot Motion Planning--IROS 2018 Workshop},
  year={2018}
}



% DQN-TAMER
  @article{DQN-TAMER-Arakawa:2018,
      title={DQN-TAMER: Human-in-the-Loop Reinforcement Learning with Intractable Feedback}, 
      author={Riku Arakawa and Sosuke Kobayashi and Yuya Unno and Yuta Tsuboi and Shin-ichi Maeda},
      year={2018},
     
      journal={arXiv preprint:1810.11748}
}

% SAIL
@article{SAIL-Xiong:2019,
	title={Learning safety-aware policy with imitation learning for context-adaptive navigation},
	author={Xiong, Bo and Wang, Fangshi and Yu, Chao and Liu, XinJun and Qiao, Fei and Yang, Yi and Wei, Qi},
	journal={CEUR Workshop Proceedings},
	year={2019}
}


% HG-DAgger

@inproceedings{HG-DAgger-Kelly:2019,
  title={Hg-dagger: Interactive imitation learning with human experts},
  author={Kelly, Michael and Sidrane, Chelsea and Driggs-Campbell, Katherine and Kochenderfer, Mykel J},
  booktitle={2019 International Conference on Robotics and Automation (ICRA)},
  pages={8077--8083},
  year={2019},
  organization={IEEE}
}




% SQIL
% @misc{SQIL-Reddy-Dragan-Levine:2019,
%       title={SQIL: Imitation Learning via Reinforcement Learning with Sparse Rewards}, 
%       author={Siddharth Reddy and Anca D. Dragan and Sergey Levine},
%       year={2019},
%       eprint={1905.11108},
%       archivePrefix={arXiv},
%       primaryClass={cs.LG}
% }


@inproceedings{SQIL-Reddy-Dragan-Levine:2019,
  author    = {Siddharth Reddy and
               Anca D. Dragan and
               Sergey Levine},
  title     = {{SQIL:} Imitation Learning via Reinforcement Learning with Sparse
               Rewards},
  booktitle = {8th International Conference on Learning Representations, {ICLR} 2020},
  year      = {2020},
  url       = {https://openreview.net/forum?id=S1xKd24twB},
  timestamp = {Thu, 07 May 2020 17:11:47 +0200},
  biburl    = {https://dblp.org/rec/conf/iclr/ReddyDL20.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}
  
% Retrospective DAgger

@article{Retrospective-DAgger-song:2019,
  title={Learning to search via retrospective imitation},
  author={Song, Jialin and Lanka, Ravi and Zhao, Albert and Bhatnagar, Aadyot and Yue, Yisong and Ono, Masahiro},
  journal={arXiv preprint:1804.00846},
  year={2018}
}

% AOR (Adaptive On-Policy Regularization)
@misc{AOR-lee-laskey:2019,
      title={Dynamic Regret Convergence Analysis and an Adaptive Regularization Algorithm for On-Policy Robot Imitation Learning}, 
      author={Jonathan N. Lee and Michael Laskey and Ajay Kumar Tanwani and Anil Aswani and Ken Goldberg},
      year={2019},
      eprint={1811.02184},
      archivePrefix={arXiv},
      primaryClass={cs.RO}
}





@inproceedings{Dynamic-regret-Laskey:2018,
  title={Stability analysis of on-policy imitation learning algorithms using dynamic regret},
  author={Lee, Jonathan and Laskey, Michael and Tanwani, Ajay Kumar and Goldberg, Ken},
  booktitle={RSS Workshop on Imitation and Causality},
  year={2018}
}
% Cycle-of-Learning

@article{Cycle-of-Learning-waytowich:2018,
  title={Cycle-of-learning for autonomous systems from human interaction},
  author={Waytowich, Nicholas R and Goecks, Vinicius G and Lawhern, Vernon J},
  journal={arXiv preprint:1808.09572},
  year={2018}
}

% EnsembleDAgger

@inproceedings{EnsembleDAgger-Menda:2019,
  title={Ensembledagger: A bayesian approach to safe imitation learning},
  author={Menda, Kunal and Driggs-Campbell, Katherine and Kochenderfer, Mykel J},
  booktitle={2019 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)},
  pages={5041--5048},
  year={2019},
  organization={IEEE}
}


% ValueDICE

@article{ValueDICE-Kostrikov:2019,
  title={Imitation learning via off-policy distribution matching},
  author={Kostrikov, Ilya and Nachum, Ofir and Tompson, Jonathan},
  journal={arXiv preprint:1912.05032},
  year={2019}
}


 %FRESH

@inproceedings{FRESH-xiao:2020,
author = {Xiao, Baicen and Lu, Qifan and Ramasubramanian, Bhaskar and Clark, Andrew and Bushnell, Linda and Poovendran, Radha},
title = {FRESH: Interactive Reward Shaping in High-Dimensional State Spaces Using Human Feedback},
year = {2020},
isbn = {9781450375184},

booktitle = {Proceedings of the 19th International Conference on Autonomous Agents and MultiAgent Systems},
pages = {1512–1520},
numpages = {9},
keywords = {human feedback, feedback-based reward shaping, deep reinforcement learning, ensemble of neural networks},

series = {AAMAS '20}
}
 
% % CSF (Converging Supervisor Framework)
% @misc{CSF-balakrishna:2020,
%       title={On-Policy Robot Imitation Learning from a Converging Supervisor}, 
%       author={Ashwin Balakrishna and Brijen Thananjeyan and Jonathan Lee and Felix Li and Arsh Zahed and Joseph E. Gonzalez and Ken Goldberg},
%       year={2020},
%       eprint={1907.03423},
%       archivePrefix={arXiv},
%       primaryClass={cs.LG}
% }






@InProceedings{CSF-balakrishna:2020,
  title = 	 {On-Policy Robot Imitation Learning from a Converging Supervisor},
  author =       {Balakrishna, Ashwin and Thananjeyan, Brijen and Lee, Jonathan and Li, Felix and Zahed, Arsh and Gonzalez, Joseph E. and Goldberg, Ken},
  booktitle = 	 {Proceedings of the Conference on Robot Learning},
  pages = 	 {24--41},
  year = 	 {2020},

  volume = 	 {100},

  month = 	 {30 Oct--01 Nov},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v100/balakrishna20a/balakrishna20a.pdf},
  url = 	 {https://proceedings.mlr.press/v100/balakrishna20a.html},
  abstract = 	 {Existing on-policy imitation learning algorithms, such as DAgger, assume access to a fixed supervisor. However, there are many settings where the supervisor may evolve during policy learning, such as a human performing a novel task or an improving algorithmic controller. We formalize imitation learning from a “converging supervisor” and provide sublinear static and dynamic regret guarantees against the best policy in hindsight with labels from the converged supervisor, even when labels during learning are only from intermediate supervisors. We then show that this framework is closely connected to a class of reinforcement learning (RL) algorithms known as dual policy iteration (DPI), which alternate between training a reactive learner with imitation learning and a model-based supervisor with data from the learner. Experiments suggest that when this framework is applied with the state-of-the-art deep model-based RL algorithm PETS as an improving supervisor, it outperforms deep RL baselines on continuous control tasks and provides up to an 80-fold speedup in policy evaluation.}
}


% ABLUF

@inproceedings{ABLUF-he:2020,
  title={Learning behaviors with uncertain human feedback},
  author={He, Xu and Chen, Haipeng and An, Bo},
  booktitle={Conference on Uncertainty in Artificial Intelligence},
  pages={131--140},
  year={2020},
  organization={PMLR}
}

% DA-RB (DAgger Replay Buffer)
@INPROCEEDINGS{DA-RB-Prakash:2020,

  author={A. {Prakash} and A. {Behl} and E. {Ohn-Bar} and K. {Chitta} and A. {Geiger}},

  booktitle={2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)}, 

  title={Exploring Data Aggregation in Policy Learning for Vision-Based Urban Autonomous Driving}, 

  year={2020},

  volume={},

  number={},

  pages={11760-11770},

  doi={10.1109/CVPR42600.2020.01178}}


% APIL
@misc{APIL-nguyen:2020,
      title={Active Imitation Learning from Multiple Non-Deterministic Teachers: Formulation, Challenges, and Algorithms}, 
      author={Khanh Nguyen and Hal Daumé III au2},
      year={2020},
      eprint={2006.07777},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

% Active Imitation Learnng (related to APIL)
@article{Active-Imitation-Learning-judah:2014,
  author  = {Kshitij Judah and Alan P. Fern and Thomas G. Dietterich and Prasad Tadepalli},
  title   = {Active Imitation Learning: Formal and Practical Reductions to I.I.D. Learning},
  journal = {Journal of Machine Learning Research},
  year    = {2014},
  volume  = {15},
  number  = {120},
  pages   = {4105-4143},
  url     = {http://jmlr.org/papers/v15/judah14a.html}
}

% EIL

@INPROCEEDINGS{EIL-Spencer:2020, 
    AUTHOR    = {Jonathan Spencer AND Sanjiban Choudhury AND Matt Barnes AND Matthew Schmittle AND Mung Chiang AND Peter Ramadge AND Siddhartha Srinivasa}, 
    TITLE     = {{Learning from Interventions: Human-robot interaction as both explicit and implicit feedback}}, 
    BOOKTITLE = {Proceedings of Robotics: Science and Systems}, 
    YEAR      = {2020}, 
    ADDRESS   = {Corvalis, Oregon, USA}, 
    MONTH     = {July}, 
    DOI       = {10.15607/RSS.2020.XVI.055} 
} 

% % FIRE
% @misc{FIRE-ablett:2020,
%       title={Fighting Failures with FIRE: Failure Identification to Reduce Expert Burden in Intervention-Based Learning}, 
%       author={Trevor Ablett and Filip Marić and Jonathan Kelly},
%       year={2020},
%       eprint={2007.00245},
%       archivePrefix={arXiv},
%       primaryClass={cs.RO}
% }



@article{FIRE-ablett:2020,
  title={Fighting Failures with FIRE: Failure Identification to Reduce Expert Burden in Intervention-Based Learning},
  author={Ablett, Trevor and Mari{\'c}, Filip and Kelly, Jonathan},
  journal={arXiv preprint:2007.00245},
  year={2020}
}


% IWR
  

@article{IWR-mandlekar:2020,
  title={Human-in-the-Loop Imitation Learning using Remote Teleoperation},
  author={Mandlekar, Ajay and Xu, Danfei and Mart{\'\i}n-Mart{\'\i}n, Roberto and Zhu, Yuke and Fei-Fei, Li and Savarese, Silvio},
  journal={arXiv preprint:2012.06733},
  year={2020}
}


% Neural Networks
@book{ANN-graupe:2013,
  title={Principles Of Artificial Neural Networks (3rd Edition)},
  author={Graupe, D.},
  isbn={9789814522755},
  series={Advanced Series In Circuits And Systems},
  url={https://books.google.es/books?id=Zz27CgAAQBAJ},
  year={2013},
  publisher={World Scientific Publishing Company}
}

% Convolutional Tikz
  
@misc{Convolutional-tikz,
      title={Drawing a convolution with Tikz}, 
      author={Herman Jaramillo},
      year={2019},
      url= {https://tex.stackexchange.com/a/579557}
}

% Neural Network Tikz
  
@misc{NN-tikz,
      title={Diagram of an artificial neural network}, 
      author={Gonzalo Medina},
      year={2013},
      url= {https://tex.stackexchange.com/a/132471}
}


% Autoencoder Tikz
  
@misc{Autoencoder-tikz,
      title={LaTeX-TikZ-Diagrams}, 
      author={James Allingham},
      year={2020},
      url= {https://github.com/JamesAllingham/LaTeX-TikZ-Diagrams}
}

% Vanilla RNN
  
@misc{Vanilla-RNN-image,
      title={Creating A Text Generator Using Recurrent Neural Network }, 
      author={Trung Tran},
      year={2020},
      url= {https://chunml.github.io/ChunML.github.io/project/Creating-Text-Generator-Using-Recurrent-Neural-Network/}
}

% LazyDagger for tree of IL > IIL
% @misc{lazydagger:2021,
%       title={LazyDAgger: Reducing Context Switching in Interactive Imitation Learning}, 
%       author={Ryan Hoque and Ashwin Balakrishna and Carl Putterman and Michael Luo and Daniel S. Brown and Daniel Seita and Brijen Thananjeyan and Ellen Novoseller and Ken Goldberg},
%       year={2021},
%       eprint={2104.00053},
%       archivePrefix={arXiv},
%       primaryClass={cs.RO}
% }

@INPROCEEDINGS{lazydagger:2021,  author={Hoque, Ryan and Balakrishna, Ashwin and Putterman, Carl and Luo, Michael and Brown, Daniel S. and Seita, Daniel and Thananjeyan, Brijen and Novoseller, Ellen and Goldberg, Ken},  booktitle={2021 IEEE 17th International Conference on Automation Science and Engineering (CASE)},   title={LazyDAgger: Reducing Context Switching in Interactive Imitation Learning},   year={2021},  volume={},  number={},  pages={502-509},  doi={10.1109/CASE49439.2021.9551469}}


% Another Laskey definition of on-policy off-policy IL

@article{OtherLaskeydefinitions:2019,
  title={Dynamic regret convergence analysis and an adaptive regularization algorithm for on-policy robot imitation learning},
  author={Lee, Jonathan N and Laskey, Michael and Tanwani, Ajay Kumar and Aswani, Anil and Goldberg, Ken},
  journal={The International Journal of Robotics Research},
  volume={40},
  number={10-11},
  pages={1284--1305},
  year={2021},
  publisher={SAGE Publications Sage UK: London, England}
}

  
%   % Another definition of on-policy off-policy IL from Berkeley
%   @misc{Anotherdefinitionfromberkeley:2020,
%       title={On-Policy Robot Imitation Learning from a Converging Supervisor}, 
%       author={Ashwin Balakrishna and Brijen Thananjeyan and Jonathan Lee and Felix Li and Arsh Zahed and Joseph E. Gonzalez and Ken Goldberg},
%       year={2020},
%       eprint={1907.03423},
%       archivePrefix={arXiv},
%       primaryClass={cs.LG}
% }

% Temporal Difference equation

@article{TD:equation:2019,
  title={Reinforcement Learning: Algorithms and Convergence},
  author={Heitzinger, Clemens},
  year={2019}
}

% Problem of collecting data
@ARTICLE{collecting_data_for_offline_learning,
  author={Wu, Qingyao and Wu, Hanrui and Zhou, Xiaoming and Tan, Mingkui and Xu, Yonghui and Yan, Yuguang and Hao, Tianyong},

  journal={IEEE Transactions on Knowledge and Data Engineering}, 
  title={Online Transfer Learning with Multiple Homogeneous or Heterogeneous Sources}, 
  year={2017},
  volume={29},
  number={7},
  pages={1494-1507},
  doi={10.1109/TKDE.2017.2685597}}
  
  
  % Artificial intelligence book
  @book{artificial_intelligence_book,
  title={Artificial Intelligence: Foundations of Computational Agents},
  author={Poole, D.L. and Mackworth, A.K.},
  isbn={9781108171021},
  url={https://books.google.nl/books?id=q6t2DwAAQBAJ},
  year={2017},
  publisher={Cambridge University Press}
}

  
% Experience replay catastrophic forgetting
@misc{catastrophic_forgetting,
      title={Memory Efficient Experience Replay for Streaming Learning}, 
      author={Tyler L. Hayes and Nathan D. Cahill and Christopher Kanan},
      year={2019},
      eprint={1809.05922},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

% Experience Replay provides stability

@inproceedings{Experience_replay_stability:2019,
 author = {Rolnick, David and Ahuja, Arun and Schwarz, Jonathan and Lillicrap, Timothy and Wayne, Gregory},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {H. Wallach and H. Larochelle and A. Beygelzimer and F. d\textquotesingle Alch\'{e}-Buc and E. Fox and R. Garnett},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {Experience Replay for Continual Learning},
 url = {https://proceedings.neurips.cc/paper/2019/file/fa7cdfad1a5aaf8370ebeda47a1ff1c3-Paper.pdf},
 volume = {32},
 year = {2019}
}


% Machine Learning for finance

@book{Machine_learning_finance:2019,
  title={Machine Learning for Finance: Principles and practice for financial insiders},
  author={Klaas, J.},
  isbn={9781789134698},
  url={https://books.google.nl/books?id=qfeaDwAAQBAJ},
  year={2019},
  publisher={Packt Publishing}
}

% Meta-World
@inproceedings{metaworld,
  title={Meta-world: A benchmark and evaluation for multi-task and meta reinforcement learning},
  author={Yu, Tianhe and Quillen, Deirdre and He, Zhanpeng and Julian, Ryan and Hausman, Karol and Finn, Chelsea and Levine, Sergey},
  booktitle={Conference on Robot Learning},
  pages={1094--1100},
  year={2020},
  organization={PMLR}
}


%Mujoco
@INPROCEEDINGS{mujoco,
  author={Todorov, Emanuel and Erez, Tom and Tassa, Yuval},
  booktitle={2012 IEEE/RSJ International Conference on Intelligent Robots and Systems}, 
  title={MuJoCo: A physics engine for model-based control}, 
  year={2012},
  volume={},
  number={},
  pages={5026-5033},
  doi={10.1109/IROS.2012.6386109}}

% Openai
@article{openai,
  title={Openai gym},
  author={Brockman, Greg and Cheung, Vicki and Pettersson, Ludwig and Schneider, Jonas and Schulman, John and Tang, Jie and Zaremba, Wojciech},
  journal={arXiv preprint:1606.01540},
  year={2016}
}


% iiwa
@article{iiwa,
  title={Towards MRI-based autonomous robotic US acquisitions: a first feasibility study},
  author={Hennersperger, Christoph and Fuerst, Bernhard and Virga, Salvatore and Zettinig, Oliver and Frisch, Benjamin and Neff, Thomas and Navab, Nassir},
  journal={IEEE transactions on medical imaging},
  volume={36},
  number={2},
  pages={538--548},
  year={2017},
  publisher={IEEE}
}

% Constant velocity problem
% @PhdThesis{constant_velocity,
%   author =       "Hogan Francois",
%   title =        "Reactive manipulation with contact models and tactile feedback",
%   school =       "Massachusetts Institute of Technology",
%   year =         "2020",
%   url = "https://dspace.mit.edu/handle/1721.1/125476",
% }


@inbook{constant_velocity,
author = {Hogan, Francois and Rodriguez, Alberto},
year = {2020},
month = {05},
pages = {800-815},
title = {Feedback Control of the Pusher-Slider System: A Story of Hybrid and Underactuated Contact Dynamics},
booktitle ={Algorithmic Foundations of Robotics XII},
isbn = {978-3-030-43088-7},
doi = {10.1007/978-3-030-43089-4_51}
}

% ThriftyDAgger

@article{ThriftyDAgger,
  title={ThriftyDAgger: Budget-Aware Novelty and Risk Gating for Interactive Imitation Learning},
  author={Hoque, Ryan and Balakrishna, Ashwin and Novoseller, Ellen and Wilcox, Albert and Brown, Daniel S and Goldberg, Ken},
  journal={arXiv preprint:2109.08273},
  year={2021}
}


% DropoutDAgger

@article{DropoutDAgger,
  title={Dropoutdagger: A bayesian approach to safe imitation learning},
  author={Menda, Kunal and Driggs-Campbell, Katherine and Kochenderfer, Mykel J},
  journal={arXiv preprint:1709.06166},
  year={2017}
}


% Learning from human preferences (Deep)
% @misc{learning-from-human-preferences:2017,
%       title={Deep reinforcement learning from human preferences}, 
%       author={Paul Christiano and Jan Leike and Tom B. Brown and Miljan Martic and Shane Legg and Dario Amodei},
%       year={2017},
%       eprint={1706.03741},
%       archivePrefix={arXiv},
%       primaryClass={stat.ML}
% }


@inproceedings{learning-from-human-preferences:2017,
 author = {Christiano, Paul F and Leike, Jan and Brown, Tom and Martic, Miljan and Legg, Shane and Amodei, Dario},
 booktitle = {Advances in Neural Information Processing Systems},

 pages = {},

 title = {Deep Reinforcement Learning from Human Preferences},
 url = {https://proceedings.neurips.cc/paper/2017/file/d5e2c0adad503c91f91df240d0cd4e49-Paper.pdf},
 volume = {30},
 year = {2017}
}


% Query demonstrations

@article{demonstration-robot-query,
  title={Interactive policy learning through confidence-based autonomy},
  author={Chernova, Sonia and Veloso, Manuela},
  journal={Journal of Artificial Intelligence Research},
  volume={34},
  pages={1--25},
  year={2009}
}


% Learning from human preferences (Atari)


@inproceedings{learning-from-human-preferences:2018,
  title={Reward learning from human preferences and demonstrations in Atari},
  author={Borja Ibarz and Jan Leike and Tobias Pohlen and Geoffrey Irving and Shane Legg and Dario Amodei},
  booktitle={NeurIPS},
  year={2018}
}

% Inverse Reinforcement Learning

% @misc{inverse-reinforcement-learning,
%       title={A Survey of Inverse Reinforcement Learning: Challenges, Methods and Progress}, 
%       author={Saurabh Arora and Prashant Doshi},
%       year={2020},
%       eprint={1806.06877},
%       archivePrefix={arXiv},
%       primaryClass={cs.LG}
% }


@article{inverse-reinforcement-learning,
title = {A survey of inverse reinforcement learning: Challenges, methods and progress},
journal = {Artificial Intelligence},
volume = {297},
pages = {103500},
year = {2021},
issn = {0004-3702},
doi = {10.1016/j.artint.2021.103500},
author = {Saurabh Arora and Prashant Doshi},
keywords = {Reinforcement learning, Reward function, Learning from demonstration, Generalization, Learning accuracy, Survey},
abstract = {Inverse reinforcement learning (IRL) is the problem of inferring the reward function of an agent, given its policy or observed behavior. Analogous to RL, IRL is perceived both as a problem and as a class of methods. By categorically surveying the extant literature in IRL, this article serves as a comprehensive reference for researchers and practitioners of machine learning as well as those new to it to understand the challenges of IRL and select the approaches best suited for the problem on hand. The survey formally introduces the IRL problem along with its central challenges such as the difficulty in performing accurate inference and its generalizability, its sensitivity to prior knowledge, and the disproportionate growth in solution complexity with problem size. The article surveys a vast collection of foundational methods grouped together by the commonality of their objectives, and elaborates how these methods mitigate the challenges. We further discuss extensions to the traditional IRL methods for handling imperfect perception, an incomplete model, learning multiple reward functions and nonlinear reward functions. The article concludes the survey with a discussion of some broad advances in the research area and currently open research questions.}
}

% Artificial neural networks


@article{ANNs,
  title={Deep learning: methods and applications},
  author={Deng, Li and Yu, Dong},
  journal={Foundations and trends in signal processing},
  volume={7},
  number={3--4},
  pages={197--387},
  year={2014},
  publisher={Now Publishers Inc. Hanover, MA, USA}
}


% % Corrections replay
% @misc{corrections-replay,
%       title={Continuous Control for High-Dimensional State Spaces: An Interactive Learning Approach}, 
%       author={Rodrigo Pérez-Dattari and Carlos Celemin and Javier Ruiz-del-Solar and Jens Kober},
%       year={2019},
%       eprint={1908.05256},
%       archivePrefix={arXiv},
%       primaryClass={cs.RO}
% }


@INPROCEEDINGS{corrections-replay,

  author={Pérez-Dattari, Rodrigo and Celemin, Carlos and Ruiz-del-Solar, Javier and Kober, Jens},

  booktitle={2019 International Conference on Robotics and Automation (ICRA)}, 

  title={Continuous Control for High-Dimensional State Spaces: An Interactive Learning Approach}, 

  year={2019},

  volume={},

  number={},

  pages={7611-7617},

  doi={10.1109/ICRA.2019.8793675}}


% Offline IL

@article{offlineIL,
  title={Mitigating Covariate Shift in Imitation Learning via Offline Data Without Great Coverage},
  author={Chang, Jonathan D and Uehara, Masatoshi and Sreenivas, Dhruv and Kidambi, Rahul and Sun, Wen},
  journal={arXiv preprint:2106.03207},
  year={2021}
}