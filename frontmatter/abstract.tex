\chapter*{Abstract}
Interactive imitation learning refers to learning methods where a human teacher interacts with an agent during the learning process providing feedback to improve its behaviour. This type of learning may be preferable with respect to reinforcement learning techniques when dealing with real-world problems. This is especially true in the case of robotic applications where there are long training times and usually, it is easier for a teacher to provide feedback rather than to specify a reward function that would lead to the desired behaviour.

\vspace{3mm} %5mm vertical space

The present thesis focuses on interactive learning with corrective feedback and, in particular, in the framework Deep Corrective Advice Communicated by Humans (D-COACH), which has successfully shown to be advantageous in terms of training time and data efficiency. D-COACH, whose policy is represented by an artificial neural network, incorporates a replay buffer where samples of states and corresponding labels gathered by the agent's policy are saved for later be replayed. However, this causes conflicts between the data in the buffer because samples collected by older versions of the policy may not be useful for updating its current version. In order to reduce these data ambiguities, the current implementation of D-COACH limits the size of its buffer. Nonetheless, this limitation propitiates catastrophic forgetting, an inherent problem of neural networks that can be mitigated by keeping the information gathered during all the stages of the problem in a replay buffer. This information is then replayed, helping the artificial neural network not to forget what it has already learnt. Therefore, D-COACH suffers from a trade-off between having a buffer small enough to avoid conflicts between the data and having a buffer big enough to avoid catastrophic forgetting. The fact that D-COACH limits the size of its buffer automatically restricts the types of problems that it can solve, given that, if the problem is too complex, it simply will not be able to remember everything.




\vspace{3mm} %5mm vertical space

If we want to utilise a buffer to train complex tasks with corrective feedback, a new method is needed to solve the problem of using information gathered by older versions of the policy. We propose an improved version of D-COACH, which we call Batch Deep COACH (BD-COACH, pronounced \say{be the coach}). BD-COACH is able to perform policy dependent batch updates by incorporating a human model module that learns the feedback from the teacher making corrections gathered by older versions of the policy, still useful for updating the current version of the policy.

\vspace{3mm} %5mm vertical space

To compare the performance of BD-COACH with respect to D-COACH, simulated experiments with three different problems were done using the open-source Meta-World benchmark, which is based on MuJoCo and OpenAI gym. Moreover, to validate the proposed method in a real setup, two planar manipulation tasks were solved using a KUKA robot arm. 

\vspace{3mm} %5mm vertical space


Furthermore, we present an analysis between on-policy and off-policy methods both in the fields of reinforcement learning and in imitation learning; we believe there is an interesting simile between this classification and the problem of correctly implementing a replay buffer.