\begin{abstract}

This document presents the preliminary literature review for the master thesis \textit{Towards Off-Policy Corrective Imitation Learning} whose objective is to improve the \textit{experience replay} implementation of the D-COACH (Deep COrrective Advice Communicated by Humans) algorithm; D-COACH is a framework in the field of Corrective Imitation Learning (CIL), where an agent is trained with feedback that a teacher provides in the form of relative corrections. Experience replay improves the sample efficiency by allowing already collected data to be used multiple times for training.  This is particularly important when training neural networks (NN) because it increases their stability helping to preserve old knowledge and thus reducing catastrophic forgetting. Furthermore, experience replay provides uncorrelated data to train the neural network which helps to generalize and to minimize overfitting. In order to use the experience replay technique, it is necessary for the algorithm to be, as it is known in reinforcement learning (RL), \textit{off-policy}. The current version of D-COACH is \textit{on-policy} and thus, the need to transform it into an off-policy version to fully leverage the benefits of experience replay. In this literature review, we first study the relevant theoretical framework, providing a general overview of reinforcement learning and what the terms on-policy and off-policy mean in that specific field. 
Then, as the main part of this review, a research in the literature is conducted for a clear definition of the terms on-policy and off-policy in imitation learning, a field where they are not as widely used as in reinforcement learning. To contribute in this aspect to the literature, two new definitions are presented and several imitation learning algorithms are classified according to these definitions. Finally, D-COACH is explained in more detail together with the proposal of how to improve the experience replay implementation by adding a new model that will predict the teacher's feedback.


% Then, I present a brief introduction to Artificial Neural Networks (ANNs) as the function approximators that are used in D-COACH including convolutional networks, autoencoders and recurrent neural networks. 
\end{abstract}