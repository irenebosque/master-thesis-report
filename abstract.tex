\begin{abstract}

Interactive learning refers to imitation learning methods where a human teacher interacts with an agent during the learning process providing feedback to improve its behaviour. This type of learning may be preferable with respect to reinforcement learning techniques in real-world problems. This is especially true in the case of robotic applications where there are long training times and where usually it is easier for a teacher to provide feedback rather than to specify a reward function that would lead to the desired behaviour.

The present thesis focuses on interactive learning with corrective feedback and in particular, in the framework called Deep Corrective Advice Communicated by Humans, D-COACH that has successfully proven to be advantageous in terms of training time and data efficiency. 

D-COACH, whose policy is represented by an artificial neural network, incorporates a replay buffer where samples gathered by the agent's policy are saved for later be replayed. However, this causes conflicts between the data in the buffer to arise because samples collected by older versions of the policy may not be useful for updating its current version.

In order to reduce the ambiguities between the data, the current implementation of D-COACH limits the size of its buffer. Nonetheless, this limitation propitiates catastrophic forgetting, an inherent problem of neural networks that makes it necessary to keep all the possible information about all the stages of the problem in the buffer to not forget what the neural network has already learnt.

Therefore, D-COACH suffers from this trade-off between having a small enough buffer to avoid conflicts between the data and having a buffer big enough to avoid catastrophic forgetting. 

The fact that D-COACH limits the size of its buffer automatically restricts the types of problems that it can solve, given that if the problem is too complex, it simply will not be able to remember everything.

If we want to utilise a buffer to train complex tasks with corrective feedback, a new method is needed to solve the problem of using information gathered by older versions of the policy. 

We propose an improved version of D-COACH, which we call Batch DCOACH (B-DCOACH, pronounced \say{be the coach}). B-DCOACH is able to perform policy dependent batch updates by incorporating a human model module that learns the feedback from the teacher making corrections gathered by older versions of the policy, still useful for updating its current version.

To compare the performance of B-DCOACH with respect to D-COACH, simulated experiments with three different problems were done using the open-source Meta-World benchmark which is based on MuJoCo and OpenAI gym. Moreover, to validate the proposed method in a real setup, two planar manipulation tasks were solved using a KUKA robot arm.

Furthermore, we present an analysis between on-policy and off-policy methods both in the fields of reinforcement learning and in imitation learning; we believe there is an interesting simile between this classification and the problem of correctly implementing a replay buffer.
\end{abstract}